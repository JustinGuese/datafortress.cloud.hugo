<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning on Datafortress.cloud</title><link>https://datafortress.cloud/tags/machine-learning/</link><description>Recent content in machine learning on Datafortress.cloud</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor/><webMaster/><lastBuildDate>Wed, 08 Feb 2023 07:10:46 +0200</lastBuildDate><atom:link href="https://datafortress.cloud/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Unlocking the Potential of Machine Learning with Private Cloud Services: Real-World Case Studies</title><link>https://datafortress.cloud/blog/case-study-real-world-applications-private-cloud/</link><pubDate>Wed, 08 Feb 2023 07:10:46 +0200</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/blog/case-study-real-world-applications-private-cloud/</guid><description>
&lt;h1 id="unlocking-the-potential-of-machine-learning-with-private-cloud-services-real-world-case-studies">Unlocking the Potential of Machine Learning with Private Cloud Services: Real-World Case Studies&lt;/h1>
&lt;p>Welcome to the world of data-driven business success! In today&amp;rsquo;s fast-paced and ever-changing business environment, companies are constantly seeking new ways to improve their operations and stay ahead of the competition. Machine learning and private cloud services have emerged as game-changers, providing businesses with the tools they need to unlock the full potential of their data. In this article, we&amp;rsquo;ll take a look at real-world examples of businesses that have harnessed the power of these cutting-edge technologies to drive growth, streamline operations, and protect sensitive information. So buckle up, and get ready to discover the many benefits of machine learning and private cloud services!&lt;/p>
&lt;h2 id="case-study-1-automated-fraud-detection-for-a-financial-services-company">Case Study 1: Automated Fraud Detection for a Financial Services Company&lt;/h2>
&lt;p>Financial services companies handle massive amounts of sensitive data on a daily basis, making fraud detection a critical component of their operations. Unfortunately, manual fraud detection processes are time-consuming, costly, and often fall short in detecting complex fraud schemes. This is where the integration of machine learning and private cloud services comes into play.&lt;/p>
&lt;p>In this case study, we&amp;rsquo;ll take a look at a financial services company that was facing challenges with its manual fraud detection processes. The company turned to DataFortress.cloud UG for a solution that could provide accurate and efficient fraud detection, while also protecting sensitive customer information.&lt;/p>
&lt;p>DataFortress.cloud UG implemented machine learning algorithms within a secure private cloud environment, to automate the fraud detection process. The results were impressive, with the financial services company experiencing a significant increase in accuracy compared to manual processes. This allowed the company to detect fraud schemes more quickly and effectively, reducing the risk of financial losses and protecting sensitive customer information.&lt;/p>
&lt;p>In conclusion, the integration of machine learning and private cloud services provides financial services companies with a powerful tool for automating fraud detection and protecting sensitive data. If you&amp;rsquo;re facing challenges with manual fraud detection processes, contact DataFortress.cloud UG to learn more about our solutions.&lt;/p>
&lt;h2 id="case-study-2-predictive-maintenance-for-a-manufacturing-company">Case Study 2: Predictive Maintenance for a Manufacturing Company&lt;/h2>
&lt;p>In the manufacturing industry, downtime can be costly and impact the bottom line. Traditional maintenance processes are reactive, meaning that equipment is only serviced after it has failed. This leads to unexpected downtime, increased maintenance costs, and decreased productivity.&lt;/p>
&lt;p>Enter predictive maintenance, a proactive approach that uses machine learning algorithms to predict when equipment will fail and schedule maintenance accordingly. In this case study, we&amp;rsquo;ll take a look at a manufacturing company that was struggling with inefficient maintenance processes and downtime.&lt;/p>
&lt;p>The manufacturing company partnered with DataFortress.cloud UG to implement predictive maintenance in a secure private cloud environment. DataFortress.cloud UG used machine learning algorithms to analyze equipment data and predict when maintenance would be necessary. This allowed the company to proactively schedule maintenance, reducing downtime and improving efficiency.&lt;/p>
&lt;p>The results were remarkable, with the manufacturing company experiencing a significant reduction in downtime and an increase in productivity. In addition, the company was able to optimize its maintenance processes and reduce costs, leading to improved profitability.&lt;/p>
&lt;p>In conclusion, predictive maintenance is a game-changer for the manufacturing industry. By using machine learning and private cloud services, companies can proactively schedule maintenance, reducing downtime and improving efficiency. If you&amp;rsquo;re facing challenges with reactive maintenance processes, contact DataFortress.cloud UG to learn more about our solutions.&lt;/p>
&lt;h2 id="case-study-3-customer-segmentation-and-personalization-for-a-retail-company">Case Study 3: Customer Segmentation and Personalization for a Retail Company&lt;/h2>
&lt;p>In today&amp;rsquo;s competitive retail landscape, providing a personalized shopping experience is key to winning and retaining customers. Customer segmentation, the process of dividing customers into groups based on common characteristics, is an essential component of personalization. But manually segmenting customers can be time-consuming and limited by human biases.&lt;/p>
&lt;p>This is where machine learning and private cloud services come into play. In this case study, we&amp;rsquo;ll take a look at a retail company that was struggling to provide personalized experiences for its customers. The company turned to DataFortress.cloud UG for a solution that could accurately segment customers and provide personalized experiences in a secure environment.&lt;/p>
&lt;p>DataFortress.cloud UG implemented machine learning algorithms in a private cloud environment to analyze customer data and segment customers into groups based on common characteristics. This allowed the retail company to provide personalized experiences for its customers, including tailored product recommendations and targeted marketing campaigns.&lt;/p>
&lt;p>The results were impressive, with the retail company experiencing an increase in customer engagement and sales. The company was also able to gain valuable insights into customer behavior and preferences, which allowed for continuous optimization and improvement of personalization efforts.&lt;/p>
&lt;p>In conclusion, customer segmentation and personalization are crucial components of a successful retail strategy. By using machine learning and private cloud services, retailers can accurately segment customers and provide personalized experiences, leading to increased engagement and sales. If you&amp;rsquo;re facing challenges with customer segmentation and personalization, contact DataFortress.cloud UG to learn more about our solutions.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In conclusion, machine learning and private cloud services are powerful tools for businesses looking to improve their operations and protect sensitive data. The case studies we&amp;rsquo;ve discussed in this article highlight just a few of the many ways in which companies are using these technologies to gain a competitive edge.&lt;/p>
&lt;p>From automating fraud detection in the financial services industry to predictive maintenance in the manufacturing industry to customer segmentation and personalization in the retail industry, the benefits of machine learning and private cloud services are clear. Businesses are able to improve efficiency, reduce costs, and provide personalized experiences for their customers, all while keeping sensitive data secure.&lt;/p>
&lt;p>At DataFortress.cloud UG, we&amp;rsquo;re dedicated to helping businesses harness the power of machine learning and private cloud services to achieve their goals. Whether you&amp;rsquo;re facing challenges with fraud detection, maintenance processes, or customer segmentation and personalization, we have the expertise and experience to help. Contact us today to learn more about our solutions and how we can help your business succeed.&lt;/p></description></item><item><title>Face Detection using MTCNN</title><link>https://datafortress.cloud/blog/face-detection-using-mtcnn/</link><pubDate>Wed, 08 Jun 2022 07:10:46 +0200</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/blog/face-detection-using-mtcnn/</guid><description>
&lt;h1 id="what-is-mtcnn">What is MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>MTCNN is a python (pip) library written by &lt;a href="https://github.com/ipazc/mtcnn">Github user ipacz&lt;/a>, which implements the [paper Zhang, Kaipeng et al. “Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.” IEEE Signal Processing Letters 23.10 (2016): 1499–1503. Crossref. Web](&lt;a href="https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)">https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)&lt;/a>.&lt;/p>
&lt;p>In this paper, they propose a deep cascaded multi-task framework using different features of “sub-models” to each boost their correlating strengths.&lt;/p>
&lt;p>MTCNN performs quite fast on a CPU, even though S3FD is still quicker running on a GPU – but that is a topic for another post.&lt;/p>
&lt;p>This post uses code from the following two sources, check them out, they are interesting as well:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/" title="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution" title="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution">https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;h1 id="basic-usage-of-mtcnn">Basic usage of MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>Feel free to access the whole notebook via:&lt;/p>
&lt;p>&lt;a href="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up" title="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up">https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up&lt;/a>&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up
&lt;/code>&lt;/pre>
&lt;p>Luckily MTCNN is available as a pip package, meaning we can easily install it using&lt;/p>
&lt;pre>&lt;code>pip install mtcnn
&lt;/code>&lt;/pre>
&lt;p>Now switching to Python/Jupyter Notebook we can check the installation with an import and quick verification:&lt;/p>
&lt;pre>&lt;code>import mtcnn
# print version
print(mtcnn.__version__)
&lt;/code>&lt;/pre>
&lt;p>Afterwards, we are ready to load out test image using the matplotlib &lt;a href="https://bit.ly/2vo3INw">imread function&lt;/a>.&lt;/p>
&lt;pre>&lt;code>import matplotlib.pyplot as plt
# load image from file
filename = &amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;
pixels = plt.imread(filename)
print(&amp;quot;Shape of image/array:&amp;quot;,pixels.shape)
imgplot = plt.imshow(pixels)
plt.show()
&lt;/code>&lt;/pre>
&lt;p>Now your output will look a lot like this:&lt;/p>
&lt;pre>&lt;code>{'box': [1942, 716, 334, 415], 'confidence': 0.9999997615814209, 'keypoints': {'left_eye': (2053, 901), 'right_eye': (2205, 897), 'nose': (2139, 976), 'mouth_left': (2058, 1029), 'mouth_right': (2206, 1023)}}
{'box': [2084, 396, 37, 46], 'confidence': 0.9999206066131592, 'keypoints': {'left_eye': (2094, 414), 'right_eye': (2112, 414), 'nose': (2102, 426), 'mouth_left': (2095, 432), 'mouth_right': (2112, 431)}}
{'box': [1980, 381, 44, 59], 'confidence': 0.9998701810836792, 'keypoints': {'left_eye': (1997, 404), 'right_eye': (2019, 407), 'nose': (2010, 417), 'mouth_left': (1995, 425), 'mouth_right': (2015, 427)}}
{'box': [2039, 395, 39, 46], 'confidence': 0.9993435740470886, 'keypoints': {'left_eye': (2054, 409), 'right_eye': (2071, 415), 'nose': (2058, 422), 'mouth_left': (2048, 425), 'mouth_right': (2065, 431)}}
&lt;/code>&lt;/pre>
&lt;p>What does this tell us? A lot of it is self-explanatory, but it basically returns coordinates, or the pixel values of a rectangle where the MTCNN algorithm detected faces. The “box” value above returns the location of the whole face, followed by a “confidence” level.&lt;/p>
&lt;p>If you want to do more advanced extractions or algorithms, you will have access to other facial landmarks, called “keypoints” as well. Namely the MTCNN model located the eyes, mouth and nose as well!&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="drawing-a-box-around-faces">Drawing a box around faces&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>To demonstrate this even better let us draw a box around the face using matplotlib:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height, fill=False, color='green')
# draw the box
ax.add_patch(rect)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index-1-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="displaying-eyes-mouth-and-nose-around-faces">Displaying eyes, mouth, and nose around faces&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Now let us take a look at the aforementioned “keypoints” that the MTCNN model returned.&lt;/p>
&lt;p>We will now use these to graph the nose, mouth, and eyes as well.&lt;br>
We will add the following code snippet to our code above:&lt;/p>
&lt;pre>&lt;code># draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='orange')
ax.add_patch(dot)
&lt;/code>&lt;/pre>
&lt;p>With the full code from above looking like this:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height,fill=False, color='orange')
# draw the box
ax.add_patch(rect)
# draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='red')
ax.add_patch(dot)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index2-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="advanced-mtcnn-speed-it-up-x100">Advanced MTCNN: Speed it up (\~x100)!&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Now let us come to the interesting part. If you are going to process millions of pictures you will need to speed up MTCNN, otherwise, you will either fall asleep or your CPU will burn before it will be done.&lt;/p>
&lt;p>But what exactly are we talking about? If you are running the above code it will take around one second, meaning we will process around one picture per second. If you are running MTCNN on a GPU and use the sped-up version it will achieve around 60-100 pictures/frames a second. That is a boost of up to &lt;strong>100 times&lt;/strong>!&lt;/p>
&lt;p>If you are for example going to extract all faces of a movie, where you will extract 10 faces per second (one second of the movie has on average around 24 frames, so every second frame) it will be 10 * 60 (seconds) * 120 (minutes) = 72,000 frames.&lt;/p>
&lt;p>Meaning if it takes one second to process one frame it will take 72,000 * 1 (seconds) = 72,000s / 60s = 1,200m = &lt;strong>20 hours&lt;/strong>&lt;/p>
&lt;p>With the sped-up version of MTCNN this task will take 72,000 (frames) / 100 (frames/sec) = 720 seconds = &lt;strong>12 minutes&lt;/strong>!&lt;/p>
&lt;p>To use MTCNN on a GPU you will need to set up CUDA, cudnn, pytorch and so on. &lt;a href="https://pytorch.org/get-started/locally/">Pytorch wrote a good tutorial about that part&lt;/a>.&lt;/p>
&lt;p>Once installed we will do the necessary imports as follows:&lt;/p>
&lt;pre>&lt;code>from facenet_pytorch import MTCNN
from PIL import Image
import torch
from imutils.video import FileVideoStream
import cv2
import time
import glob
from tqdm.notebook import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu'
filenames = [&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;,&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>See how we defined the device in the code above? You will be able to run everything on a CPU as well if you do not want or can set up CUDA.&lt;/p>
&lt;p>Next, we will define the extractor:&lt;/p>
&lt;pre>&lt;code># define our extractor
fast_mtcnn = FastMTCNN(
stride=4,
resize=0.5,
margin=14,
factor=0.6,
keep_all=True,
device=device
)
&lt;/code>&lt;/pre>
&lt;p>In this snippet, we pass along some parameters, where we for example only use half of the image size, which is one of the main impact factors for speeding it up.&lt;/p>
&lt;p>And finally, let us run the face extraction script:&lt;/p>
&lt;pre>&lt;code>def run_detection(fast_mtcnn, filenames):
frames = []
frames_processed = 0
faces_detected = 0
batch_size = 60
start = time.time()
for filename in tqdm(filenames):
v_cap = FileVideoStream(filename).start()
v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))
for j in range(v_len):
frame = v_cap.read()
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
frames.append(frame)
if len(frames) &amp;gt;= batch_size or j == v_len - 1:
faces = fast_mtcnn(frames)
frames_processed += len(frames)
faces_detected += len(faces)
frames = []
print(
f'Frames per second: {frames_processed / (time.time() - start):.3f},',
f'faces detected: {faces_detected}\r',
end=''
)
v_cap.stop()
run_detection(fast_mtcnn, filenames)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/teslap100frames.webp" alt="">&lt;/p>
&lt;p>The above image shows the output of the code running on an NVIDIA Tesla P100, so depending on the source material, GPU and processor you might experience better or worse performance.&lt;/p>
&lt;!-- raw HTML omitted --></description></item><item><title>How to deploy an automated trading bot using the Facebook Prophet Machine Learning model to AWS Lambda (serverless)</title><link>https://datafortress.cloud/blog/how-to-deploy-an-automated-trading-bot-using-the-facebook-prophet-machine-learning-model-to-aws-lambda-serverless/</link><pubDate>Mon, 23 May 2022 22:00:00 +0000</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/blog/how-to-deploy-an-automated-trading-bot-using-the-facebook-prophet-machine-learning-model-to-aws-lambda-serverless/</guid><description>
&lt;p>I divided this post into the “Why did I do it” and the “Technical How To”. If you want to skip the “Why” part, feel free to directly jump to the Technical part.&lt;/p>
&lt;h1 id="why-should-i-deploy-a-machine-learning-model-in-aws-lambda">Why should I deploy a machine learning model in AWS Lambda?&lt;/h1>
&lt;p>&lt;strong>1. Reliability:&lt;/strong> The algorithm will execute independently of other systems, updates, …&lt;/p>
&lt;p>&lt;strong>2. Performance Efficiency:&lt;/strong> I can run several algorithms on one (small) system, independent from each other.&lt;/p>
&lt;p>&lt;strong>3. Cost Savings:&lt;/strong> AWS allows for &lt;a href="https://aws.amazon.com/lambda/?did=ft_card&amp;amp;trk=ft_card">3,2 million compute-seconds&lt;/a> per month, basically letting me run all my algorithms for free.&lt;/p>
&lt;p>I have been searching for a way to first make sure my investment bot surely executes because a failed execution might cost a lot of money if a trade is not canceled promptly if it goes in the wrong direction. Additionally, I wanted to avoid letting my computer run all the time and to make sure several algorithms could run next to each other, without influencing or delaying their execution.&lt;/p>
&lt;p>Furthermore, it is a nice thought to have an investing algorithm run without worrying about operating system updates, hardware failures, and power cuts, etc, which is the general advantage of serverless technologies.&lt;/p>
&lt;p>Right now, I can run several variations of the algorithm to test out alterations of the algorithm and can be sure that it will run. Another nice thing? AWS offers around 1 Million free Lambda calls, which lets me run the whole architecture in its free tier contingent.&lt;/p>
&lt;h2 id="the-investing-algorithm">The investing algorithm&lt;/h2>
&lt;p>I am going to explain the algorithm in more depth in another post on my website &lt;a href="http://www.datafortress.cloud">www.datafortress.cloud&lt;/a>, but my typical investment algorithm setup consists of:&lt;/p>
&lt;ol>
&lt;li>Testing the algorithm using &lt;a href="https://www.backtrader.com/">Backtrader&lt;/a>, an open-source backtesting framework written in python&lt;/li>
&lt;li>Converting the successful algorithm into a single python file containing a run() method that returns which investments have been done&lt;/li>
&lt;li>Transferring the python file to AWS Lambda, where I am calling the run() function with AWS Lambda’s lambda_handler function&lt;/li>
&lt;/ol>
&lt;p>In this example algorithm, I take investment decisions depending on if the current price is above or below the trendline predicted by &lt;a href="https://facebook.github.io/prophet/">Facebook’s prophet model&lt;/a>. I have &lt;a href="http://seangtkelley.me/blog/2018/08/15/algo-trading-pt2">taken ideas from Sean Kelley&lt;/a>, who wrote a Backtrader setup on how to utilize prophet with Backtrader.&lt;/p>
&lt;p>My stock universe in this setup is calculated by choosing the top 20 stocks out of the SPY500 index, which achieved the highest return in the past X timesteps.&lt;/p>
&lt;p>The data source is Yahoo finance, using the &lt;a href="https://pypi.org/project/yfinance/">free yfinance library&lt;/a>, and as my algorithmic broker of choice, I have chosen &lt;a href="https://alpaca.markets/">Alpaca.markets&lt;/a>.&lt;/p>
&lt;p>In my setup, the algorithm will execute once per day at 3 p.m. or every 15 minutes during trading hours.&lt;/p>
&lt;h3 id="the-problems-deploying-facebook-prophet-to-aws-lambda">The problems deploying Facebook Prophet to AWS Lambda&lt;/h3>
&lt;p>AWS Lambda comes with some python libraries preinstalled, but as many of you might know, this is by default quite limited (which is reasonable for Lambda’s promise). Still, Lambda allows for private packages to be installed which is quite easy for smaller packages (see the &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/python-package.html">official documentation&lt;/a>) but becomes a little more complicated if dealing with packages that exceed 250 Mb in size. Unfortunately, Facebook’s prophet model exceeds this boundary, but luckily &lt;a href="https://towardsdatascience.com/how-to-get-fbprophet-work-on-aws-lambda-c3a33a081aaf">Alexandr Matsenov solved this issue by reducing the package size&lt;/a> and &lt;a href="https://github.com/marcmetz/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda">Marc Metz handled compilation issues to make it run on AWS Lambda&lt;/a>.&lt;/p>
&lt;p>Non-default libraries can be added to AWS Lambda by using Layers, which contain all the packages needed. If a layer is imported, you can simply import the packages in your python function as you would do it in your local setup.&lt;/p>
&lt;h2 id="how-to-technical">How to (technical)&lt;/h2>
&lt;p>Finally, let me explain how exactly you can achieve this. See this TLDR for the impatient types, or the more detailed version below.&lt;/p>
&lt;p>&lt;strong>TLDR;&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>You will need a Lambda Layer, upload mine (&lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/raw/master/python.zip">download&lt;/a>) containing Prophet, yfinance, … to an S3 bucket (private access)&lt;/li>
&lt;li>Select AWS Lambda, create a function, add a layer and paste in your S3 object URL&lt;/li>
&lt;li>Paste your lambda_function.py into the Lambda Editor (&lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/blob/master/lambda_function.py">or use mine&lt;/a>)&lt;/li>
&lt;li>Set up your Environment variables (optional)&lt;/li>
&lt;li>Either run it manually by clicking “test” or head over to CloudWatch -&amp;gt; Rules -&amp;gt; Create Rule and set up “Schedule Execution” to run it in a specified time interval&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Detailed Explanation&lt;/strong>:&lt;/p>
&lt;h3 id="1-creating-a-custom-layer-for-aws-lambda">1. Creating a custom layer for AWS Lambda&lt;/h3>
&lt;p>You can either use my Lambda layer containing Facebook Prophet, NumPy, pandas, &lt;a href="https://github.com/alpacahq/alpaca-trade-api-python">alpaca-trading-API&lt;/a>, yfinance (&lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda">GitHub&lt;/a>) or compile your own using the explanation given by &lt;a href="https://medium.com/@marc.a.metz/docker-run-rm-it-v-pwd-var-task-lambci-lambda-build-python3-7-bash-c7d53f3b7eb2">Marc&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Using my Lambda Layer&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>Download the zip file from my &lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/raw/master/python.zip">Github repo&lt;/a> containing all packages (&lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/raw/master/python.zip">Link&lt;/a>).&lt;/li>
&lt;li>As you can only directly upload layers to Lambda until the size of 50 Mb, we will first need to upload the file to AWS S3.&lt;/li>
&lt;li>Create a bucket and place the downloaded zip file into it. Access can remain private and does NOT need to be public! Copy the URL to your file (e.g. &lt;a href="https://BUCKETNAME.s3.REGION.amazonaws.com/python.zip" title="https://BUCKETNAME.s3.REGION.amazonaws.com/python.zip">https://BUCKETNAME.s3.REGION.amazonaws.com/python.zip&lt;/a>).&lt;/li>
&lt;li>Log into AWS and go to Lambda -&amp;gt; Layers (&lt;a href="https://eu-central-1.console.aws.amazon.com/lambda/home?region=eu-central-1#/layers">EU central Link&lt;/a>).&lt;/li>
&lt;li>Click “Create layer”, give it a matching name and select “Upload a file from Amazon S3”, and copy the code of step 3 into it. As Runtimes select Python 3.7. Click create.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Compiling your own Lambda Layer&lt;/strong>&lt;/p>
&lt;p>Please &lt;a href="https://medium.com/@marc.a.metz/docker-run-rm-it-v-pwd-var-task-lambci-lambda-build-python3-7-bash-c7d53f3b7eb2">follow the instructions of Marc&lt;/a>.&lt;/p>
&lt;h3 id="2-setting-up-an-aws-lambda-function">2. Setting up an AWS Lambda function&lt;/h3>
&lt;ol>
&lt;li>Open the Lambda Function Dashboard (&lt;a href="https://eu-central-1.console.aws.amazon.com/lambda/home?region=eu-central-1#/functions">EU central Link&lt;/a>) and click “Create function”&lt;/li>
&lt;li>Leave the “Author from scratch” checkbox as is and give it a fitting name.&lt;/li>
&lt;li>In “Runtime”, select Python 3.7, leave the rest as is and click “Create function”.&lt;/li>
&lt;li>In the overview of the “designer” tab, you will see a graphical representation of your Lambda function. Click on the “layers” box below it and click “Add a layer”. If you correctly set up the layer, you will be able to select it in the following dialogue. Finally, click on “Add”.&lt;/li>
&lt;li>In the “designer” tab, select your Lambda Function. If you scroll down, you will see a default python code snippet in a file called “lambda_function.py”. If you have structured your code the same as mine (&lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/blob/master/lambda_function.py">Link&lt;/a>), you can execute your function with the run() function. If a Lambda function is called, it will execute the lambda_handler(event, context) function from which you could e.g. call the run() function. Of course, you can rename all files and functions, but for the simplicity of this project, I left it as it is.&lt;/li>
&lt;li>Feel free to just paste in &lt;a href="https://github.com/JustinGuese/How-To-Deploy-Facebook-Prophet-on-AWS-Lambda/blob/master/lambda_function.py">my function&lt;/a> and test it.&lt;/li>
&lt;li>Clicking on “Test” should result in successful execution, otherwise, it will state the errors in the dialogue.&lt;/li>
&lt;/ol>
&lt;h3 id="3-using-environment-variables-in-aws-lambda">3. Using environment variables in AWS Lambda&lt;/h3>
&lt;p>You should never leave your user and password as cleartext in your code, which is why you should always use environment variables! Luckily, Lambda uses them as well, and they can easily be called with the python os package. E.g. in my script I am calling the user variable with os.environ[&amp;lsquo;ALPACAUSER&amp;rsquo;]. The environment variables can be set up in the main Lambda function screen when scrolling down below your code editor.&lt;/p>
&lt;h3 id="4-trigger-aws-lambda-functions-at-a-specified-time-interval">4. Trigger AWS Lambda functions at a specified time interval&lt;/h3>
&lt;p>The concept of serverless and AWS Lambda is built on the idea that a function is executed when a trigger event happens. In my setup, I wanted the function to be called e.g. every 15 minutes during trading hours, Monday to Friday. Luckily, AWS offers a way to trigger an event without the need to run a server, using the CloudWatch service.&lt;/p>
&lt;ol>
&lt;li>Head over to CloudWatch (&lt;a href="https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1">EU central Link&lt;/a>).&lt;/li>
&lt;li>In the left panel, select “Events” and “Rules”.&lt;/li>
&lt;li>Click on “Create Rule”, and select “Schedule” instead of “Event pattern”. Here you can use the simple “Fixed-rate” dialogue, or create a cron expression. I am using &lt;a href="https://crontab.guru/" title="https://crontab.guru/">https://crontab.guru/&lt;/a> (free) to create cron expressions. My cron expression for the abovementioned use case is “0/15 13-21 ? * MON-FRI *”.&lt;/li>
&lt;li>In the right panel, select “Add Target” and select your Lambda function. It will automatically be added to Lambda.&lt;/li>
&lt;li>Finally click on “Configure details”, give it a name, and click on “Create rule”.&lt;/li>
&lt;/ol>
&lt;h3 id="5-optional-log-analysis-error-search">5. (optional) Log Analysis, Error Search&lt;/h3>
&lt;p>If you have made it to this part, you should be done! But if you want to check if everything worked, you can use CloudWatch to have a look at the outputs of the Lambda functions. Head over to CloudWatch -&amp;gt; Logs -&amp;gt; Log groups (&lt;a href="https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1#logsV2:log-groups">EU central Link&lt;/a>) and select your Lambda function. In this overview, you should be able to see the output of your functions.&lt;/p>
&lt;p>If you have liked this post leave a comment or head over to my blog &lt;a href="http://www.datafortress.cloud">www.datafortress.cloud&lt;/a> to keep me motivated 😊.&lt;/p></description></item><item><title>How To - Ditching Ubuntu in favor of Arch Linux for a Deep Learning Workstation</title><link>https://datafortress.cloud/blog/howto-arch-linux-deeplearning-workstation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/blog/howto-arch-linux-deeplearning-workstation/</guid><description>
&lt;h1 id="how-to-ditching-ubuntu-in-favor-of-arch-linux-for-a-deep-learning-workstation">How To: Ditching Ubuntu in favor of Arch Linux for a Deep Learning Workstation&lt;/h1>
&lt;h2 id="why-should-i-ditch-ubuntu">Why should I ditch Ubuntu?&lt;/h2>
&lt;p>Most of you might be using Ubuntu for their workstations, and that is fine for the more inexperienced users. One of the issues I had with Ubuntu and the Tensorflow/CUDA though, has been that handling the different drivers and versions of CUDA, cudnn, TensorFlow, and so on has been quite a struggle. I’m not sure about you, but once I had a working Tensorflow 1.15 or 2.0 environment, I usually did not touch it anymore being scared to mess up this holy configuration.&lt;/p>
&lt;p>Working with different programs it would be nice to have a way of switching between the two most used TensorFlow versions of 1.15 and 2.0 like you can do with Google Colab in a single command, but installing a different TensorFlow version usually messed up my system again.&lt;/p>
&lt;p>Additionally, Arch has always been on my To-Do list, as it is the most “barebone” Linux distro you can get, meaning you are working way closer on the hardware compared to “higher abstractions” like Ubuntu. In their own words, Ubuntu is built to “work out of the box and make the installation process as easy as possible for new users”, whilst the motto of Arch Linux is “customize everything”.
Being way closer to the hardware Arch is insanely faster compared to Ubuntu (and miles ahead of Windows), for the cost of more Terminal usage.&lt;/p>
&lt;p>When I have been using Arch in the past weeks, RAM usage usually halved compared to Ubuntu, and installing Machine Learning packages is a breeze. I can have both TensorFlow 1.15 and 2.0 working together, switching the versions with Anaconda environments. Also, the system works quite stable, as I am using the LTS (long term support) kernels of Linux, and usually updates to the famous AUR (user-made packages in Arch) are coming out a month ahead of the Debian (Ubuntu) packages.&lt;/p>
&lt;p>All in all, I can only recommend setting up an Arch Linux Deep Learning station as it is:&lt;/p>
&lt;ol>
&lt;li>Faster, like packages will install super fast, deep learning is supercharged, &amp;hellip;&lt;/li>
&lt;li>More stable&lt;/li>
&lt;li>Easier to switch between TensorFlow versions
compared to Ubuntu.&lt;/li>
&lt;/ol>
&lt;p>I will split the how-to in two parts, the first one being “How to I install Arch Linux” and the second one being “How to install the Deep Learning workstation packages”.&lt;/p>
&lt;p>For the general &lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">“How to install Arch Linux”, head over to this article&lt;/a>.&lt;/p>
&lt;p>If Arch is too complex for now, you could try out &lt;a href="//manjaro.org/">Manjaro&lt;/a>, which is a user-friendly version of Arch, even though I can not guarantee that all packages will work the same, as they are slightly different. All in all it should work the same though.&lt;/p>
&lt;p>I was thinking about creating a ready to install Image (iso or img), if enough people are interested leave a comment below or message me!&lt;/p>
&lt;h2 id="installing-the-deep-learning-tensorflow-cuda-cudnn-anaconda-setup-on-a-fresh-arch-linux-installation">Installing the Deep Learning (TensorFlow, CUDA, CUDNN, Anaconda) setup on a fresh Arch Linux installation&lt;/h2>
&lt;p>Once you are &lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">done with the Arch installation (phew!)&lt;/a>, let us first change some settings such that our system works more stable.&lt;/p>
&lt;h3 id="1-switching-to-the-fastest-mirrors">1. Switching to the fastest mirrors&lt;/h3>
&lt;p>Software is downloaded from so-called “mirrors”, which are servers containing all the Arch libraries. If not done automatically, it could happen that your servers are not optimized yet. Therefore, we are going to install a small tool that finds and saves the fastest servers called “reflector”&lt;/p>
&lt;p>Install reflector using&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S reflector&lt;/p>
&lt;/blockquote>
&lt;p>Find and download the best servers&lt;/p>
&lt;blockquote>
&lt;p>reflector &amp;ndash;verbose -l 20 -n 20 &amp;ndash;sort rate &amp;ndash;save /etc/pacman.d/mirrorlist&lt;/p>
&lt;/blockquote>
&lt;p>Check the output if it makes sense, e.g. if the domains are close to your location. If not, you could add the country tag to get more precise results, e.g. for Germany and Austria:&lt;/p>
&lt;blockquote>
&lt;p>reflector -c “AT,DE” &amp;ndash;verbose -l 20 -n 20 &amp;ndash;sort rate &amp;ndash;save /etc/pacman.d/mirrorlist&lt;/p>
&lt;/blockquote>
&lt;p>Update your installation&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -Syyu&lt;/p>
&lt;/blockquote>
&lt;h3 id="2-changing-the-desktop-environment">2. Changing the Desktop Environment&lt;/h3>
&lt;p>If you are using Manjaro or chose the “Gnome” Desktop environment as you know it from Ubuntu, it might be worth it to think about changing it as Gnome is known to eat more RAM than Chrome, and we surely need RAM in our Deep Learning setup.&lt;/p>
&lt;p>If you like Gnome, feel free to skip this step. Otherwise, I can recommend the Xfce desktop as it is a good combination of lightweight and full of features.&lt;/p>
&lt;p>Download Xfce&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S xfce4 xfce4-goodies lxdm&lt;/p>
&lt;/blockquote>
&lt;p>Lxdm is a display manager that allows you to use multiple desktops.&lt;/p>
&lt;p>Log out of your current session and press Alt + F2 (or Alt + F3 if it does not work) to get a terminal. First disable Gnome and afterward “activate” Xfce:&lt;/p>
&lt;p>Deactivate and uninstall gnome:&lt;/p>
&lt;blockquote>
&lt;p>sudo systemctl disable gdm &lt;br>
sudo pacman -R gnome gnome-extras&lt;/p>
&lt;/blockquote>
&lt;p>Activate Xfce&lt;/p>
&lt;blockquote>
&lt;p>sudo systemctl enable lxdm &lt;br>
sudo systemctl start lxdm&lt;/p>
&lt;/blockquote>
&lt;p>If the new Xfce desktop does open just login and explore, if not try to reboot (sudo reboot). If that does not help proceed to crying and rolling on the floor, and send me a message or comment afterward.&lt;/p>
&lt;h3 id="3-installing-the-lts-long-term-support-linux-kernels-for-better-stability">3. Installing the LTS (long term support) Linux kernels for better stability&lt;/h3>
&lt;p>Arch is famous for being really close to the current Linux kernels, which is good if you always want the newest packages and Linux features, but a bad idea if you are building a Deep Learning Workstation.&lt;/p>
&lt;p>That is why I switched to the LTS kernels, which are basically kernels that receive more support and are more stable than the newer versions of the Linux Kernel.&lt;/p>
&lt;p>Luckily switching kernels is super easy in Arch. First we will download the kernels, and afterward tell our boot manager which kernel to choose.&lt;/p>
&lt;p>First download the LTS kernels:&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S linux-lts linux-lts-headers&lt;/p>
&lt;/blockquote>
&lt;p>Have a look at your current kernel versions:&lt;/p>
&lt;blockquote>
&lt;p>ls -lsha /boot&lt;/p>
&lt;/blockquote>
&lt;p>One kernel should be named vmlinuz-linux.img and initramfs-linux.img (your current versions) and the LTS ones the same with -lts at the end.&lt;/p>
&lt;p>If you are seeing two kernels you can now proceed to delete the old kernels:&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -R linux&lt;/p>
&lt;/blockquote>
&lt;p>Now a more advanced part is that you will need to tell your bootloader which kernel to choose. The question is which bootloader you are using, but in most cases it is Grub. If you followed my Arch installation tutorial your bootloader is systemd-boot.&lt;/p>
&lt;p>My recommendation is try the Grub instructions, and if that does not work proceed to the others.&lt;/p>
&lt;h4 id="changing-the-grub-bootloader-for-the-lts-linux-kernels">Changing the Grub bootloader for the LTS linux kernels&lt;/h4>
&lt;blockquote>
&lt;p>grub-mkconfig -o /boot/grub/grub.cfg&lt;/p>
&lt;/blockquote>
&lt;p>If you see an error proceed to the next bootloader, otherwise reboot (sudo reboot).&lt;/p>
&lt;h4 id="changing-the-syslinux-bootloader-for-the-lts-linux-kernels">Changing the syslinux bootloader for the LTS linux kernels&lt;/h4>
&lt;p>Edit the config file:&lt;/p>
&lt;blockquote>
&lt;p>sudo nano /boot/syslinux/syslinux.cfg&lt;/p>
&lt;/blockquote>
&lt;p>Simply add “-lts” to the vmlinuz-linux.img and initramfs-linux.img, such that they are vmlinuz-linux-lts.img and initramfs-linux-lts.img&lt;/p>
&lt;h4 id="changing-the-systemd-boot-bootloader-for-the-lts-linux-kernels">Changing the systemd-boot bootloader for the LTS linux kernels&lt;/h4>
&lt;p>If you are coming from my Arch installation guide, this is your bootloader.&lt;/p>
&lt;p>Edit the config file:&lt;/p>
&lt;blockquote>
&lt;p>sudo nano /boot/loader/entries/arch.conf&lt;/p>
&lt;/blockquote>
&lt;p>Simply add “-lts” to the vmlinuz-linux.img and initramfs-linux.img, such that they are vmlinuz-linux-lts.img and initramfs-linux-lts.img&lt;/p>
&lt;h3 id="4-installing-yay-an-easy-way-to-install-aur-packages">4. Installing yay, an easy way to install AUR packages&lt;/h3>
&lt;p>You should prefer to use the ultra-fast pacman to install most packages, but an amazing thing about Arch is that users create millions of custom packages that are super easy to install. You can basically find any program you can think of in this repo.&lt;/p>
&lt;p>Install git SVC&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S git &lt;br>
mkdir ~/tmp &lt;br>
git clone &lt;a href="https://aur.archlinux.org/yay-git.git">https://aur.archlinux.org/yay-git.git&lt;/a> ~/tmp/yay &lt;br>
cd ~/tmp/yay &lt;br>
makepkg -si&lt;/p>
&lt;/blockquote>
&lt;p>Now you can browse all the nice AUR packages in &lt;a href="https://aur.archlinux.org/packages/">https://aur.archlinux.org/packages/&lt;/a> or just go for it and type:&lt;/p>
&lt;blockquote>
&lt;p>yay -S [PACKAGE]&lt;/p>
&lt;/blockquote>
&lt;p>To install it.&lt;/p>
&lt;h3 id="5-finally-the-real-cuda-cudnn-anaconda-installation-running-both-tensorflow-115-and-20">5. Finally, the real cuda, cudnn, anaconda installation running both TensorFlow 1.15 and 2.0&lt;/h3>
&lt;p>Install Nvidia drivers, cuda, cudnn with a simple command&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S nvidia nvidia-utils cuda cudnn&lt;/p>
&lt;/blockquote>
&lt;p>This takes some time, so grab a coffee or proceed with the next steps&lt;/p>
&lt;p>Download Anaconda, I like Miniconda:&lt;/p>
&lt;blockquote>
&lt;p>wget &lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh">https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&lt;/a> ~/&lt;/p>
&lt;/blockquote>
&lt;p>Make it executable and install&lt;/p>
&lt;blockquote>
&lt;p>cd ~/ &lt;br>
chmod +x ./Miniconda*.sh &lt;br>
./Miniconda*.sh&lt;/p>
&lt;/blockquote>
&lt;p>Just leave everything as default.&lt;/p>
&lt;blockquote>
&lt;p>source ./bash_profile&lt;/p>
&lt;/blockquote>
&lt;p>Reboot your system&lt;/p>
&lt;blockquote>
&lt;p>sudo reboot&lt;/p>
&lt;/blockquote>
&lt;p>Install tensorflow&lt;/p>
&lt;p>Now is the time to decide between TensorFlow for CPU or GPU. I will continue with the GPU option, but if you want to run the CPU version just remove the “-gpu” from the package name.&lt;/p>
&lt;h5 id="create-an-anaconda-environment-for-tensorflow-20">Create an anaconda environment for Tensorflow 2.0&lt;/h5>
&lt;blockquote>
&lt;p>conda create &amp;ndash;name tf2.0 &lt;br>
conda activate tf2.0 &lt;br>
conda install pip &lt;br>
conda install tensorflow-gpu pandas numpy&lt;/p>
&lt;/blockquote>
&lt;p>Done! Now check the result with:&lt;/p>
&lt;blockquote>
&lt;p>python &lt;br>
from tensorflow.python.client import device_lib &lt;br>
device_lib.list_local_devices()&lt;/p>
&lt;/blockquote>
&lt;p>If the result shows a device name like this you are done!&lt;/p>
&lt;p>2018-05-01 05:25:25.929575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 3080 10GB major: …&lt;/p>
&lt;h5 id="create-an-anaconda-environment-for-tensorflow-115">Create an anaconda environment for Tensorflow 1.15&lt;/h5>
&lt;blockquote>
&lt;p>conda deactivate &lt;br>
conda create &amp;ndash;name tf1.15 &lt;br>
conda activate tf1.15 &lt;br>
conda install pip python==3.7 &lt;br>
conda install tensorflow-gpu==1.15&lt;/p>
&lt;/blockquote>
&lt;p>And again check if everything works and your gpu is recognized:&lt;/p>
&lt;blockquote>
&lt;p>python &lt;br>
from tensorflow.python.client import device_lib &lt;br>
device_lib.list_local_devices()&lt;/p>
&lt;/blockquote>
&lt;h3 id="6-switching-between-tensorflow-115-and-tensorflow-20-on-one-device">6. Switching between TensorFlow 1.15 and TensorFlow 2.0 on one device!&lt;/h3>
&lt;p>Just a dream coming true in my opinion, just select the 1.15 version with&lt;/p>
&lt;blockquote>
&lt;p>conda activate tf1.15&lt;/p>
&lt;/blockquote>
&lt;p>And the TensorFlow 2.0 version with&lt;/p>
&lt;blockquote>
&lt;p>conda activate tf2.0&lt;/p>
&lt;/blockquote></description></item></channel></rss>