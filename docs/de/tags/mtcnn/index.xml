<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MTCNN on Datafortress.cloud</title><link>https://datafortress.cloud/de/tags/mtcnn/</link><description>Recent content in MTCNN on Datafortress.cloud</description><generator>Hugo -- gohugo.io</generator><language>de-de</language><managingEditor/><webMaster/><lastBuildDate>Sun, 08 May 2022 07:10:46 +0200</lastBuildDate><atom:link href="https://datafortress.cloud/de/tags/mtcnn/index.xml" rel="self" type="application/rss+xml"/><item><title>Gesichtserkennung mittels MTCNN</title><link>https://datafortress.cloud/de/blog/face-detection-using-mtcnn/</link><pubDate>Sun, 08 May 2022 07:10:46 +0200</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/de/blog/face-detection-using-mtcnn/</guid><description>
&lt;h1 id="was-ist-mtcnn">Was ist MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>MTCNN ist eine Python (pip)-Bibliothek, die von [Github-Benutzer ipacz] (&lt;a href="https://github.com/ipazc/mtcnn">https://github.com/ipazc/mtcnn&lt;/a>) geschrieben wurde und die [das Papier Zhang, Kaipeng et al. &amp;ldquo;Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks&amp;rdquo; implementiert. IEEE Signal Processing Letters 23.10 (2016): 1499-1503. Querverweis. Web](&lt;a href="https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)">https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)&lt;/a>.&lt;/p>
&lt;p>In diesem Papier schlagen sie einen tief kaskadierten Multi-Task-Rahmen vor, der verschiedene Merkmale von &amp;ldquo;Untermodellen&amp;rdquo; verwendet, um jeweils ihre korrelierenden Stärken zu verstärken.&lt;/p>
&lt;p>MTCNN ist auf einer CPU recht schnell, obwohl S3FD auf einer GPU immer noch schneller läuft - aber das ist ein Thema für einen anderen Beitrag.&lt;/p>
&lt;p>Dieser Beitrag verwendet Code aus den beiden folgenden Quellen, schauen Sie sich diese an, sie sind ebenfalls interessant:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/" title="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution" title="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution">https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;h1 id="grundlegende-verwendung-von-mtcnn">Grundlegende Verwendung von MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>Zögern Sie nicht, auf das gesamte Notebook zuzugreifen:&lt;/p>
&lt;p>&lt;a href="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up" title="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up">https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up&lt;/a>&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up
&lt;/code>&lt;/pre>
&lt;p>Glücklicherweise ist MTCNN als Pip-Paket erhältlich, was bedeutet, dass wir es leicht installieren können mit&lt;/p>
&lt;pre>&lt;code>pip install mtcnn
&lt;/code>&lt;/pre>
&lt;p>Wenn wir jetzt zu Python/Jupyter Notebook wechseln, können wir die Installation mit einem Import und einer schnellen Überprüfung überprüfen:&lt;/p>
&lt;pre>&lt;code>import mtcnn
# print version
print(mtcnn.__version__)
&lt;/code>&lt;/pre>
&lt;p>Danach sind wir bereit, das Testbild mit der Matplotlib [imread-Funktion] (&lt;a href="https://bit.ly/2vo3INw">https://bit.ly/2vo3INw&lt;/a>) auszuladen.&lt;/p>
&lt;pre>&lt;code>import matplotlib.pyplot as plt
# load image from file
filename = &amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;
pixels = plt.imread(filename)
print(&amp;quot;Shape of image/array:&amp;quot;,pixels.shape)
imgplot = plt.imshow(pixels)
plt.show()
&lt;/code>&lt;/pre>
&lt;p>Nun wird Ihre Ausgabe in etwa so aussehen:&lt;/p>
&lt;pre>&lt;code>{'box': [1942, 716, 334, 415], 'Vertrauen': 0,999999997615814209, 'Schlüsselpunkte': {'linkes_Auge': (2053, 901), &amp;quot;rechtes_Auge&amp;quot;: (2205, 897), &amp;quot;Nase&amp;quot;: (2139, 976), &amp;quot;Mund_links&amp;quot;: (2139, 976), &amp;quot;Mund_links&amp;quot;: (2058, 1029), 'Mund_rechts': (2058, 1029), 'Mund_rechts': (2206, 1023)}}
{&amp;quot;Kiste&amp;quot;: [2084, 396, 37, 46], 'Vertrauen': 0,9999206066131592, 'Schlüsselpunkte': {'linkes_Auge': [2084, 396, 37, 46], 'Vertrauen': [2084, 396, 37, 46], 'Vertrauen': 0,99999206066131592, 'Schlüsselpunkte': [0,9999206066131592], 'Vertrauen (2094, 414), &amp;quot;rechtes_Auge&amp;quot;: (2094, 414), &amp;quot;rechtes_Auge&amp;quot;: (2112, 414), &amp;quot;Nase&amp;quot;: (2094, 414), &amp;quot;Nase&amp;quot;: (2102, 426), &amp;quot;Mund_links&amp;quot;: (2095, 432), &amp;quot;Mund_rechts&amp;quot;: (2112, 431)}}
{&amp;quot;Kiste&amp;quot;: [1980, 381, 44, 59], 'Vertrauen': 0,9998701810836792, 'Schlüsselpunkte': {'linkes_Auge': [1980, 381, 44, 59], 'Vertrauen': 0,9998701810836792, 'Schlüsselpunkte': [1980, 381, 44, 59]: (1997, 404), &amp;quot;rechtes_Auge&amp;quot;: (2019, 407), &amp;quot;Nase&amp;quot;: (1997, 404), &amp;quot;Nase&amp;quot;: (2010, 417), &amp;quot;Mund_links&amp;quot;: (2010, 417), &amp;quot;Mund_links&amp;quot;: (1995, 425), &amp;quot;Mund_rechts&amp;quot;: (1995, 425), &amp;quot;Mund_rechts&amp;quot;: (2015, 427)}}
{&amp;quot;Kiste&amp;quot;: [2039, 395, 39, 46], 'Vertrauen': 0,9993435740470886, 'Schlüsselpunkte': {'linkes_Auge': [2039, 395, 39, 46], 'Vertrauen': 0,9993435740470886, 'Vertrauen': 0,9993435740470886, 'Schlüsselpunkte': [2039, 395, 39, 46]: (2054, 409), &amp;quot;rechtes_Auge&amp;quot;: (2054, 409), &amp;quot;rechtes_Auge&amp;quot;: (2071, 415), &amp;quot;Nase&amp;quot;: (2054, 409), &amp;quot;Nase&amp;quot;: (2058, 422), &amp;quot;Mund_links&amp;quot;: (2048, 425), 'Mund_rechts': (2048, 425), 'Mund_rechts': (2065, 431)}}
&lt;/code>&lt;/pre>
&lt;p>Was sagt uns das? Vieles davon ist selbsterklärend, aber im Grunde liefert es Koordinaten oder die Pixelwerte eines Rechtecks, in dem der MTCNN-Algorithmus Gesichter erkannt hat. Der obige &amp;ldquo;Kasten&amp;rdquo;-Wert gibt die Position des gesamten Gesichts zurück, gefolgt von einem &amp;ldquo;Vertrauens&amp;rdquo;-Level.&lt;/p>
&lt;p>Wenn Sie fortgeschrittenere Extraktionen oder Algorithmen durchführen möchten, haben Sie auch Zugang zu anderen Landmarken des Gesichts, die als &amp;ldquo;Schlüsselpunkte&amp;rdquo; bezeichnet werden. Das MTCNN-Modell lokalisierte nämlich auch die Augen, den Mund und die Nase!&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="zeichnen-eines-kastens-um-gesichter">Zeichnen eines Kastens um Gesichter&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Um dies noch besser zu demonstrieren, zeichnen wir mit matplotlib einen Kasten um das Gesicht:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height, fill=False, color='green')
# draw the box
ax.add_patch(rect)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index-1-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="darstellung-von-augen-mund-und-nase-um-gesichter">Darstellung von Augen, Mund und Nase um Gesichter&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Werfen wir nun einen Blick auf die oben erwähnten &amp;ldquo;Schlüsselpunkte&amp;rdquo;, die das MTCNN-Modell zurückgebracht hat.&lt;/p>
&lt;p>Wir werden diese nun auch für die Darstellung von Nase, Mund und Augen verwenden.&lt;br>
Wir werden den folgenden Codeschnipsel zu unserem obigen Code hinzufügen:&lt;/p>
&lt;pre>&lt;code># draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='orange')
ax.add_patch(dot)
&lt;/code>&lt;/pre>
&lt;p>Mit dem vollständigen Code von oben, der wie folgt aussieht:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height,fill=False, color='orange')
# draw the box
ax.add_patch(rect)
# draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='red')
ax.add_patch(dot)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index2-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="erweitertes-mtcnn-beschleunigen-sie-es-x100">Erweitertes MTCNN: Beschleunigen Sie es (\\~x100)!&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Kommen wir nun zum interessanten Teil. Wenn Sie Millionen von Bildern verarbeiten wollen, müssen Sie MTCNN beschleunigen, sonst werden Sie entweder einschlafen oder Ihre CPU wird verbrennen, bevor sie fertig ist.&lt;/p>
&lt;p>Aber worüber genau reden wir hier? Wenn Sie den obigen Code ausführen, dauert es etwa eine Sekunde, d.h. wir werden etwa ein Bild pro Sekunde verarbeiten. Wenn Sie MTCNN auf einer GPU ausführen und die beschleunigte Version verwenden, werden etwa 60-100 Bilder pro Sekunde erreicht. Das ist eine Steigerung von bis zu &lt;strong>100 Mal&lt;/strong>!&lt;/p>
&lt;p>Wenn Sie z.B. alle Gesichter eines Films extrahieren wollen, wobei Sie 10 Gesichter pro Sekunde extrahieren (eine Sekunde des Films hat im Durchschnitt etwa 24 Bilder, also jedes zweite Bild), dann sind es 10 * 60 (Sekunden) * 120 (Minuten) = 72.000 Bilder.&lt;/p>
&lt;p>Das heißt, wenn die Verarbeitung eines Einzelbildes eine Sekunde dauert, dauert sie 72.000 * 1 (Sekunden) = 72.000s / 60s = 1.200m = &lt;strong>20 Stunden&lt;/strong>.&lt;/p>
&lt;p>Mit der Beschleunigungsversion von MTCNN wird diese Aufgabe 72.000 (Frames) / 100 (Frames/sec) = 720 Sekunden = &lt;strong>12 Minuten&lt;/strong> dauern!&lt;/p>
&lt;p>Um MTCNN auf einer GPU zu verwenden, müssen Sie CUDA, cudnn, pytorch usw. einrichten. &lt;a href="https://pytorch.org/get-started/locally/">Pytorch hat ein gutes Tutorial zu diesem Teil geschrieben&lt;/a>.&lt;/p>
&lt;p>Nach der Installation werden wir die notwendigen Importe wie folgt durchführen:&lt;/p>
&lt;pre>&lt;code>from facenet_pytorch import MTCNN
from PIL import Image
import torch
from imutils.video import FileVideoStream
import cv2
import time
import glob
from tqdm.notebook import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu'
filenames = [&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;,&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Sehen Sie, wie wir das Gerät im obigen Code definiert haben. Sie können alles auch auf einer CPU laufen lassen, wenn Sie CUDA nicht einrichten wollen oder können.&lt;/p>
&lt;p>Als nächstes werden wir den Extraktor definieren:&lt;/p>
&lt;pre>&lt;code># define our extractor
fast_mtcnn = FastMTCNN(
stride=4,
resize=0.5,
margin=14,
factor=0.6,
keep_all=True,
device=device
)
&lt;/code>&lt;/pre>
&lt;p>In diesem Schnipsel geben wir einige Parameter weiter, wobei wir zum Beispiel nur die halbe Bildgröße verwenden, was einer der Haupteinflussfaktoren für die Beschleunigung ist.&lt;/p>
&lt;p>Und schließlich lassen wir das Skript zur Gesichtsextraktion laufen:&lt;/p>
&lt;pre>&lt;code>def run_detection(fast_mtcnn, filenames):
frames = []
frames_processed = 0
faces_detected = 0
batch_size = 60
start = time.time()
for filename in tqdm(filenames):
v_cap = FileVideoStream(filename).start()
v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))
for j in range(v_len):
frame = v_cap.read()
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
frames.append(frame)
if len(frames) &amp;gt;= batch_size or j == v_len - 1:
faces = fast_mtcnn(frames)
frames_processed += len(frames)
faces_detected += len(faces)
frames = []
print(
f'Frames per second: {frames_processed / (time.time() - start):.3f},',
f'faces detected: {faces_detected}\r',
end=''
)
v_cap.stop()
run_detection(fast_mtcnn, filenames)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/teslap100frames.webp" alt="">&lt;/p>
&lt;p>Das obige Bild zeigt die Ausgabe des Codes, der auf einem NVIDIA Tesla P100 läuft. Je nach Quellmaterial, Grafikprozessor und Prozessor kann die Leistung also besser oder schlechter ausfallen.&lt;/p>
&lt;p>&lt;a href="https://www.datafortress.cloud/de/contact/">Sie haben eine ähnliche Idee oder wir haben Ihr Interesse geweckt? Kontaktieren Sie uns jetzt für eine gratis 15-minütige Beratung!&lt;/a>&lt;/p></description></item></channel></rss>