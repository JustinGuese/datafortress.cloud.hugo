<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>big data on Datafortress.cloud</title><link>https://datafortress.cloud/de/categories/big-data/</link><description>Recent content in big data on Datafortress.cloud</description><generator>Hugo -- gohugo.io</generator><language>de-de</language><managingEditor/><webMaster/><lastBuildDate>Sun, 08 May 2022 07:10:46 +0200</lastBuildDate><atom:link href="https://datafortress.cloud/de/categories/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Gesichtserkennung mittels MTCNN</title><link>https://datafortress.cloud/de/blog/face-detection-using-mtcnn/</link><pubDate>Sun, 08 May 2022 07:10:46 +0200</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/de/blog/face-detection-using-mtcnn/</guid><description>
&lt;h1 id="was-ist-mtcnn">Was ist MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>MTCNN ist eine Python (pip)-Bibliothek, die von [Github-Benutzer ipacz] (&lt;a href="https://github.com/ipazc/mtcnn">https://github.com/ipazc/mtcnn&lt;/a>) geschrieben wurde und die [das Papier Zhang, Kaipeng et al. &amp;ldquo;Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks&amp;rdquo; implementiert. IEEE Signal Processing Letters 23.10 (2016): 1499-1503. Querverweis. Web](&lt;a href="https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)">https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)&lt;/a>.&lt;/p>
&lt;p>In diesem Papier schlagen sie einen tief kaskadierten Multi-Task-Rahmen vor, der verschiedene Merkmale von &amp;ldquo;Untermodellen&amp;rdquo; verwendet, um jeweils ihre korrelierenden Stärken zu verstärken.&lt;/p>
&lt;p>MTCNN ist auf einer CPU recht schnell, obwohl S3FD auf einer GPU immer noch schneller läuft - aber das ist ein Thema für einen anderen Beitrag.&lt;/p>
&lt;p>Dieser Beitrag verwendet Code aus den beiden folgenden Quellen, schauen Sie sich diese an, sie sind ebenfalls interessant:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/" title="https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/">https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution" title="https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution">https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;h1 id="grundlegende-verwendung-von-mtcnn">Grundlegende Verwendung von MTCNN&lt;/h1>
&lt;!-- raw HTML omitted -->
&lt;p>Zögern Sie nicht, auf das gesamte Notebook zuzugreifen:&lt;/p>
&lt;p>&lt;a href="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up" title="https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up">https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up&lt;/a>&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up
&lt;/code>&lt;/pre>
&lt;p>Glücklicherweise ist MTCNN als Pip-Paket erhältlich, was bedeutet, dass wir es leicht installieren können mit&lt;/p>
&lt;pre>&lt;code>pip install mtcnn
&lt;/code>&lt;/pre>
&lt;p>Wenn wir jetzt zu Python/Jupyter Notebook wechseln, können wir die Installation mit einem Import und einer schnellen Überprüfung überprüfen:&lt;/p>
&lt;pre>&lt;code>import mtcnn
# print version
print(mtcnn.__version__)
&lt;/code>&lt;/pre>
&lt;p>Danach sind wir bereit, das Testbild mit der Matplotlib [imread-Funktion] (&lt;a href="https://bit.ly/2vo3INw">https://bit.ly/2vo3INw&lt;/a>) auszuladen.&lt;/p>
&lt;pre>&lt;code>import matplotlib.pyplot as plt
# load image from file
filename = &amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;
pixels = plt.imread(filename)
print(&amp;quot;Shape of image/array:&amp;quot;,pixels.shape)
imgplot = plt.imshow(pixels)
plt.show()
&lt;/code>&lt;/pre>
&lt;p>Nun wird Ihre Ausgabe in etwa so aussehen:&lt;/p>
&lt;pre>&lt;code>{'box': [1942, 716, 334, 415], 'Vertrauen': 0,999999997615814209, 'Schlüsselpunkte': {'linkes_Auge': (2053, 901), &amp;quot;rechtes_Auge&amp;quot;: (2205, 897), &amp;quot;Nase&amp;quot;: (2139, 976), &amp;quot;Mund_links&amp;quot;: (2139, 976), &amp;quot;Mund_links&amp;quot;: (2058, 1029), 'Mund_rechts': (2058, 1029), 'Mund_rechts': (2206, 1023)}}
{&amp;quot;Kiste&amp;quot;: [2084, 396, 37, 46], 'Vertrauen': 0,9999206066131592, 'Schlüsselpunkte': {'linkes_Auge': [2084, 396, 37, 46], 'Vertrauen': [2084, 396, 37, 46], 'Vertrauen': 0,99999206066131592, 'Schlüsselpunkte': [0,9999206066131592], 'Vertrauen (2094, 414), &amp;quot;rechtes_Auge&amp;quot;: (2094, 414), &amp;quot;rechtes_Auge&amp;quot;: (2112, 414), &amp;quot;Nase&amp;quot;: (2094, 414), &amp;quot;Nase&amp;quot;: (2102, 426), &amp;quot;Mund_links&amp;quot;: (2095, 432), &amp;quot;Mund_rechts&amp;quot;: (2112, 431)}}
{&amp;quot;Kiste&amp;quot;: [1980, 381, 44, 59], 'Vertrauen': 0,9998701810836792, 'Schlüsselpunkte': {'linkes_Auge': [1980, 381, 44, 59], 'Vertrauen': 0,9998701810836792, 'Schlüsselpunkte': [1980, 381, 44, 59]: (1997, 404), &amp;quot;rechtes_Auge&amp;quot;: (2019, 407), &amp;quot;Nase&amp;quot;: (1997, 404), &amp;quot;Nase&amp;quot;: (2010, 417), &amp;quot;Mund_links&amp;quot;: (2010, 417), &amp;quot;Mund_links&amp;quot;: (1995, 425), &amp;quot;Mund_rechts&amp;quot;: (1995, 425), &amp;quot;Mund_rechts&amp;quot;: (2015, 427)}}
{&amp;quot;Kiste&amp;quot;: [2039, 395, 39, 46], 'Vertrauen': 0,9993435740470886, 'Schlüsselpunkte': {'linkes_Auge': [2039, 395, 39, 46], 'Vertrauen': 0,9993435740470886, 'Vertrauen': 0,9993435740470886, 'Schlüsselpunkte': [2039, 395, 39, 46]: (2054, 409), &amp;quot;rechtes_Auge&amp;quot;: (2054, 409), &amp;quot;rechtes_Auge&amp;quot;: (2071, 415), &amp;quot;Nase&amp;quot;: (2054, 409), &amp;quot;Nase&amp;quot;: (2058, 422), &amp;quot;Mund_links&amp;quot;: (2048, 425), 'Mund_rechts': (2048, 425), 'Mund_rechts': (2065, 431)}}
&lt;/code>&lt;/pre>
&lt;p>Was sagt uns das? Vieles davon ist selbsterklärend, aber im Grunde liefert es Koordinaten oder die Pixelwerte eines Rechtecks, in dem der MTCNN-Algorithmus Gesichter erkannt hat. Der obige &amp;ldquo;Kasten&amp;rdquo;-Wert gibt die Position des gesamten Gesichts zurück, gefolgt von einem &amp;ldquo;Vertrauens&amp;rdquo;-Level.&lt;/p>
&lt;p>Wenn Sie fortgeschrittenere Extraktionen oder Algorithmen durchführen möchten, haben Sie auch Zugang zu anderen Landmarken des Gesichts, die als &amp;ldquo;Schlüsselpunkte&amp;rdquo; bezeichnet werden. Das MTCNN-Modell lokalisierte nämlich auch die Augen, den Mund und die Nase!&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="zeichnen-eines-kastens-um-gesichter">Zeichnen eines Kastens um Gesichter&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Um dies noch besser zu demonstrieren, zeichnen wir mit matplotlib einen Kasten um das Gesicht:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height, fill=False, color='green')
# draw the box
ax.add_patch(rect)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index-1-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="darstellung-von-augen-mund-und-nase-um-gesichter">Darstellung von Augen, Mund und Nase um Gesichter&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Werfen wir nun einen Blick auf die oben erwähnten &amp;ldquo;Schlüsselpunkte&amp;rdquo;, die das MTCNN-Modell zurückgebracht hat.&lt;/p>
&lt;p>Wir werden diese nun auch für die Darstellung von Nase, Mund und Augen verwenden.&lt;br>
Wir werden den folgenden Codeschnipsel zu unserem obigen Code hinzufügen:&lt;/p>
&lt;pre>&lt;code># draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='orange')
ax.add_patch(dot)
&lt;/code>&lt;/pre>
&lt;p>Mit dem vollständigen Code von oben, der wie folgt aussieht:&lt;/p>
&lt;pre>&lt;code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height,fill=False, color='orange')
# draw the box
ax.add_patch(rect)
# draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='red')
ax.add_patch(dot)
# show the plot
plt.show()
# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/index2-150x150.webp" alt="">&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="erweitertes-mtcnn-beschleunigen-sie-es-x100">Erweitertes MTCNN: Beschleunigen Sie es (\\~x100)!&lt;/h2>
&lt;!-- raw HTML omitted -->
&lt;p>Kommen wir nun zum interessanten Teil. Wenn Sie Millionen von Bildern verarbeiten wollen, müssen Sie MTCNN beschleunigen, sonst werden Sie entweder einschlafen oder Ihre CPU wird verbrennen, bevor sie fertig ist.&lt;/p>
&lt;p>Aber worüber genau reden wir hier? Wenn Sie den obigen Code ausführen, dauert es etwa eine Sekunde, d.h. wir werden etwa ein Bild pro Sekunde verarbeiten. Wenn Sie MTCNN auf einer GPU ausführen und die beschleunigte Version verwenden, werden etwa 60-100 Bilder pro Sekunde erreicht. Das ist eine Steigerung von bis zu &lt;strong>100 Mal&lt;/strong>!&lt;/p>
&lt;p>Wenn Sie z.B. alle Gesichter eines Films extrahieren wollen, wobei Sie 10 Gesichter pro Sekunde extrahieren (eine Sekunde des Films hat im Durchschnitt etwa 24 Bilder, also jedes zweite Bild), dann sind es 10 * 60 (Sekunden) * 120 (Minuten) = 72.000 Bilder.&lt;/p>
&lt;p>Das heißt, wenn die Verarbeitung eines Einzelbildes eine Sekunde dauert, dauert sie 72.000 * 1 (Sekunden) = 72.000s / 60s = 1.200m = &lt;strong>20 Stunden&lt;/strong>.&lt;/p>
&lt;p>Mit der Beschleunigungsversion von MTCNN wird diese Aufgabe 72.000 (Frames) / 100 (Frames/sec) = 720 Sekunden = &lt;strong>12 Minuten&lt;/strong> dauern!&lt;/p>
&lt;p>Um MTCNN auf einer GPU zu verwenden, müssen Sie CUDA, cudnn, pytorch usw. einrichten. &lt;a href="https://pytorch.org/get-started/locally/">Pytorch hat ein gutes Tutorial zu diesem Teil geschrieben&lt;/a>.&lt;/p>
&lt;p>Nach der Installation werden wir die notwendigen Importe wie folgt durchführen:&lt;/p>
&lt;pre>&lt;code>from facenet_pytorch import MTCNN
from PIL import Image
import torch
from imutils.video import FileVideoStream
import cv2
import time
import glob
from tqdm.notebook import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu'
filenames = [&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;,&amp;quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Sehen Sie, wie wir das Gerät im obigen Code definiert haben. Sie können alles auch auf einer CPU laufen lassen, wenn Sie CUDA nicht einrichten wollen oder können.&lt;/p>
&lt;p>Als nächstes werden wir den Extraktor definieren:&lt;/p>
&lt;pre>&lt;code># define our extractor
fast_mtcnn = FastMTCNN(
stride=4,
resize=0.5,
margin=14,
factor=0.6,
keep_all=True,
device=device
)
&lt;/code>&lt;/pre>
&lt;p>In diesem Schnipsel geben wir einige Parameter weiter, wobei wir zum Beispiel nur die halbe Bildgröße verwenden, was einer der Haupteinflussfaktoren für die Beschleunigung ist.&lt;/p>
&lt;p>Und schließlich lassen wir das Skript zur Gesichtsextraktion laufen:&lt;/p>
&lt;pre>&lt;code>def run_detection(fast_mtcnn, filenames):
frames = []
frames_processed = 0
faces_detected = 0
batch_size = 60
start = time.time()
for filename in tqdm(filenames):
v_cap = FileVideoStream(filename).start()
v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))
for j in range(v_len):
frame = v_cap.read()
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
frames.append(frame)
if len(frames) &amp;gt;= batch_size or j == v_len - 1:
faces = fast_mtcnn(frames)
frames_processed += len(frames)
faces_detected += len(faces)
frames = []
print(
f'Frames per second: {frames_processed / (time.time() - start):.3f},',
f'faces detected: {faces_detected}\r',
end=''
)
v_cap.stop()
run_detection(fast_mtcnn, filenames)
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://datafortress.cloud/images/teslap100frames.webp" alt="">&lt;/p>
&lt;p>Das obige Bild zeigt die Ausgabe des Codes, der auf einem NVIDIA Tesla P100 läuft. Je nach Quellmaterial, Grafikprozessor und Prozessor kann die Leistung also besser oder schlechter ausfallen.&lt;/p>
&lt;p>&lt;a href="https://www.datafortress.cloud/de/contact/">Sie haben eine ähnliche Idee oder wir haben Ihr Interesse geweckt? Kontaktieren Sie uns jetzt für eine gratis 15-minütige Beratung!&lt;/a>&lt;/p></description></item><item><title>How To - Weg mit Ubuntu zugunsten von Arch Linux für eine Deep Learning-optimierte Arbeitsumgebung</title><link>https://datafortress.cloud/de/blog/anleitung-wie-bauche-ich-eine-arch-linux-machine-learning-workstation/</link><pubDate>Fri, 29 Apr 2022 07:10:46 +0200</pubDate><author>Justin Guese</author><guid>https://datafortress.cloud/de/blog/anleitung-wie-bauche-ich-eine-arch-linux-machine-learning-workstation/</guid><description>
&lt;h1 id="how-to---weg-mit-ubuntu-zugunsten-von-arch-linux-für-eine-machine-learning-optimierte-arbeitsumgebung">How To - Weg mit Ubuntu zugunsten von Arch Linux für eine Machine Learning-optimierte Arbeitsumgebung&lt;/h1>
&lt;h2 id="warum-sollte-ich-ubuntu-ersetzen">Warum sollte ich Ubuntu ersetzen?&lt;/h2>
&lt;p>Die meisten von Ihnen verwenden vielleicht Ubuntu für ihre Workstations, und das ist für die unerfahreneren Benutzer in Ordnung. Eines der Probleme, die ich jedoch mit Ubuntu und Tensorflow/CUDA hatte, war, dass der Umgang mit den verschiedenen Treibern und Versionen von CUDA, cudnn, TensorFlow und so weiter ein ziemlicher Kampf war. Bei Ihnen bin ich mir nicht sicher, aber sobald ich eine funktionierende Tensorflow 1.15 oder 2.0-Umgebung hatte, habe ich sie normalerweise nicht mehr angefasst, weil ich Angst hatte, diese heilige Konfiguration durcheinander zu bringen.&lt;/p>
&lt;p>Wenn man mit verschiedenen Programmen arbeitet, wäre es schön, eine Möglichkeit zu haben, zwischen den beiden meistgenutzten TensorFlow Versionen 1.15 und 2.0 zu wechseln, wie man es mit Google Colab mit einem einzigen Befehl tun kann, aber die Installation einer anderen TensorFlow Version hat mein System normalerweise wieder durcheinander gebracht.&lt;/p>
&lt;p>Außerdem stand Arch schon immer auf meiner To-Do-Liste, da es die beste &amp;ldquo;Barebone&amp;rdquo;-Linux-Distribution ist, die man bekommen kann, was bedeutet, dass man im Vergleich zu &amp;ldquo;höheren Abstraktionen&amp;rdquo; wie Ubuntu viel näher an der Hardware arbeitet. Nach ihren eigenen Worten ist Ubuntu dafür gebaut, &amp;ldquo;out of the box zu arbeiten und den Installationsprozess für neue Benutzer so einfach wie möglich zu machen&amp;rdquo;, während das Motto von Arch Linux &amp;ldquo;alles anpassen&amp;rdquo; lautet.
Da Arch viel näher an der Hardware ist, ist es im Vergleich zu Ubuntu wahnsinnig schneller (und Windows meilenweit voraus), und das bei den Kosten für mehr Terminal-Nutzung.&lt;/p>
&lt;p>Wenn ich Arch in den letzten Wochen benutzt habe, hat sich die RAM-Nutzung normalerweise im Vergleich zu Ubuntu halbiert, und die Installation von Machine-Learning-Paketen ist ein Kinderspiel. Ich kann sowohl TensorFlow 1.15 als auch 2.0 zusammenarbeiten lassen, indem ich die Versionen mit Anaconda-Umgebungen austausche. Außerdem arbeitet das System recht stabil, da ich die LTS-Kernel (Long Term Support) von Linux verwende und Aktualisierungen der berühmten AUR-Pakete (User Made Packages in Arch) normalerweise einen Monat vor den Debian-Paketen (Ubuntu) herauskommen.&lt;/p>
&lt;p>Alles in allem kann ich nur empfehlen, eine Arch-Linux-Deep-Learning-Station so einzurichten, wie sie ist:&lt;/p>
&lt;ol>
&lt;li>Schneller, so wie sich Pakete superschnell installieren lassen, ist tiefes Lernen aufgeladen, &amp;hellip;&lt;/li>
&lt;li>Stabiler&lt;/li>
&lt;li>Einfacherer Wechsel zwischen TensorFlow Versionen
im Vergleich zu Ubuntu.&lt;/li>
&lt;/ol>
&lt;p>Ich werde die Anleitung in zwei Teile aufteilen, wobei der erste Teil &amp;ldquo;&lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">Wie installiere ich Arch Linux&lt;/a>&amp;rdquo; und der zweite Teil &amp;ldquo;&lt;a href="//www.datafortress.cloud/blog/howto-arch-linux-deeplearning-workstation/">Wie installiere ich die Deep-Learning-Workstation-Pakete&lt;/a>&amp;rdquo; lautet.&lt;/p>
&lt;p>Für das allgemeine &lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">&amp;ldquo;Wie man Arch Linux installiert&amp;rdquo;, gehen Sie zu diesem Artikel&lt;/a>.&lt;/p>
&lt;p>Wenn Arch für den Moment zu komplex ist, könnten Sie &lt;a href="//manjaro.org/">Manjaro&lt;/a> ausprobieren, eine benutzerfreundliche Version von Arch, auch wenn ich nicht garantieren kann, dass alle Pakete gleich funktionieren, da sie leicht unterschiedlich sind. Alles in allem sollte es aber gleich funktionieren.&lt;/p>
&lt;p>Ich habe darüber nachgedacht, ein installationsfertiges Image (iso oder img) zu erstellen, wenn genügend Leute daran interessiert sind, hinterlassen Sie einen Kommentar unten oder schreiben Sie mir eine Nachricht!&lt;/p>
&lt;h2 id="installation-des-deep-learning-tensorflow-cuda-cudnn-anaconda-setups-auf-einer-frischen-arch-linux-installation">Installation des Deep Learning (TensorFlow, CUDA, CUDNN, Anaconda) Setups auf einer frischen Arch-Linux-Installation&lt;/h2>
&lt;p>Wenn Sie &lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">mit der Installation von Arch (puh!)&lt;/a> fertig sind, lassen Sie uns zunächst einige Einstellungen so ändern, dass unser System stabiler arbeitet.&lt;/p>
&lt;h3 id="1-umschalten-auf-die-schnellsten-spiegel">1. Umschalten auf die schnellsten Spiegel&lt;/h3>
&lt;p>Software wird von so genannten &amp;ldquo;Mirrors&amp;rdquo; heruntergeladen, das sind Server, die alle Arch-Bibliotheken enthalten. Wenn dies nicht automatisch geschieht, kann es passieren, dass Ihre Server noch nicht optimiert sind. Deshalb werden wir ein kleines Tool installieren, das die schnellsten Server findet und speichert, den sogenannten &amp;ldquo;Spiegel&amp;rdquo;.&lt;/p>
&lt;p>Installieren Sie den Reflektor mit&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S reflector&lt;/p>
&lt;/blockquote>
&lt;p>Finden Sie die besten Server und laden Sie sie herunter&lt;/p>
&lt;blockquote>
&lt;p>reflector &amp;ndash;verbose -l 20 -n 20 &amp;ndash;sort rate &amp;ndash;save /etc/pacman.d/mirrorlist&lt;/p>
&lt;/blockquote>
&lt;p>Prüfen Sie die Ausgabe, ob sie sinnvoll ist, z.B. wenn die Domains in der Nähe Ihres Standortes liegen. Wenn nicht, können Sie das Länderkennzeichen hinzufügen, um genauere Ergebnisse zu erhalten, z.B. für Deutschland und Österreich:&lt;/p>
&lt;blockquote>
&lt;p>reflector -c “AT,DE” &amp;ndash;verbose -l 20 -n 20 &amp;ndash;sort rate &amp;ndash;save /etc/pacman.d/mirrorlist&lt;/p>
&lt;/blockquote>
&lt;p>Aktualisieren Sie Ihre Installation&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -Syyu&lt;/p>
&lt;/blockquote>
&lt;h3 id="2-ändern-der-desktop-umgebung">2. Ändern der Desktop-Umgebung&lt;/h3>
&lt;p>Wenn Sie Manjaro verwenden oder die &amp;ldquo;Gnome&amp;rdquo;-Desktop-Umgebung wählen, wie Sie sie von Ubuntu her kennen, könnte es sich lohnen, darüber nachzudenken, sie zu ändern, da Gnome bekanntermaßen mehr RAM als Chrome frisst, und wir in unserem Deep Learning-Setup sicherlich RAM benötigen.&lt;/p>
&lt;p>Wenn Ihnen Gnome gefällt, können Sie diesen Schritt gerne überspringen. Ansonsten kann ich den Xfce-Desktop empfehlen, da er eine gute Kombination aus geringem Gewicht und vielen Funktionen ist.&lt;/p>
&lt;p>Xfce herunterladen&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S xfce4 xfce4-goodies lxdm&lt;/p>
&lt;/blockquote>
&lt;p>Lxdm ist ein Displaymanager, mit dem Sie mehrere Desktops verwenden können.&lt;/p>
&lt;p>Melden Sie sich von Ihrer aktuellen Sitzung ab und drücken Sie Alt + F2 (oder Alt + F3, wenn es nicht funktioniert), um ein Terminal zu erhalten. Deaktivieren Sie zuerst Gnome und &amp;ldquo;aktivieren&amp;rdquo; Sie danach Xfce:&lt;/p>
&lt;blockquote>
&lt;p>sudo systemctl disable gdm &lt;br>
sudo pacman -R gnome gnome-extras&lt;/p>
&lt;/blockquote>
&lt;p>Aktiviere Xfce&lt;/p>
&lt;blockquote>
&lt;p>sudo systemctl enable lxdm &lt;br>
sudo systemctl start lxdm&lt;/p>
&lt;/blockquote>
&lt;p>Wenn die neue Xfce-Arbeitsoberfläche geöffnet wird, melden Sie sich einfach an und erkunden Sie sie, wenn nicht, versuchen Sie einen Neustart (sudo reboot). Wenn das nicht hilft, fahren Sie fort zu weinen und sich auf dem Boden zu wälzen, und senden Sie mir danach eine Nachricht oder einen Kommentar.&lt;/p>
&lt;h3 id="3-installation-der-lts-langzeit-unterstützung-linux-kernel-für-bessere-stabilität">3. Installation der LTS (Langzeit-Unterstützung) Linux-Kernel für bessere Stabilität&lt;/h3>
&lt;p>Arch ist berühmt dafür, dass er den aktuellen Linux-Kerneln sehr nahe kommt, was gut ist, wenn Sie immer die neuesten Pakete und Linux-Funktionen wollen, aber eine schlechte Idee, wenn Sie eine Deep Learning Workstation bauen.&lt;/p>
&lt;p>Deshalb bin ich auf die LTS-Kernel umgestiegen, die im Grunde Kernel sind, die mehr Unterstützung erhalten und stabiler sind als die neueren Versionen des Linux-Kernels.&lt;/p>
&lt;p>Zum Glück ist der Wechsel des Kernels in Arch. Zuerst werden wir die Kernel herunterladen und danach unserem Bootmanager mitteilen, welchen Kernel er wählen soll.&lt;/p>
&lt;p>Zuerst laden wir die LTS-Kernel herunter:&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S linux-lts linux-lts-headers&lt;/p>
&lt;/blockquote>
&lt;p>Werfen Sie einen Blick auf Ihre aktuellen Kernel-Versionen:&lt;/p>
&lt;blockquote>
&lt;p>ls -lsha /boot&lt;/p>
&lt;/blockquote>
&lt;p>Ein Kernel sollte vmlinuz-linux.img und initramfs-linux.img (Ihre aktuellen Versionen) heißen und die LTS-Kernel die gleichen mit -lts am Ende.&lt;/p>
&lt;p>Wenn Sie zwei Kernel sehen, können Sie nun damit fortfahren, die alten Kernel zu löschen:&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -R linux&lt;/p>
&lt;/blockquote>
&lt;p>Ein fortgeschrittener Teil ist nun, dass Sie Ihrem Bootloader mitteilen müssen, welchen Kernel er wählen soll. Die Frage ist, welchen Bootloader Sie verwenden, aber in den meisten Fällen ist es Grub. Wenn Sie &lt;a href="//www.datafortress.cloud/blog/howto-install-arch-linux-the-easy-way/">meinem Arch-Installations-Tutorial gefolgt sind&lt;/a>, ist Ihr Bootloader systemd-boot.&lt;/p>
&lt;p>Meine Empfehlung ist, die Grub-Anweisungen auszuprobieren, und wenn das nicht funktioniert, fahren Sie mit den anderen fort.&lt;/p>
&lt;h4 id="ändern-des-grub-bootloaders-für-die-lts-linux-kernel">Ändern des Grub-Bootloaders für die LTS-Linux-Kernel&lt;/h4>
&lt;blockquote>
&lt;p>grub-mkconfig -o /boot/grub/grub.cfg&lt;/p>
&lt;/blockquote>
&lt;p>Wenn Sie einen Fehler sehen, fahren Sie mit dem nächsten Bootloader fort, andernfalls führen Sie einen Neustart (sudo reboot) durch.&lt;/p>
&lt;h4 id="ändern-des-syslinux-bootloaders-für-die-lts-linux-kernel">Ändern des syslinux-Bootloaders für die LTS-Linux-Kernel&lt;/h4>
&lt;p>Bearbeiten Sie die Konfigurationsdatei:&lt;/p>
&lt;blockquote>
&lt;p>sudo nano /boot/syslinux/syslinux.cfg&lt;/p>
&lt;/blockquote>
&lt;p>Fügen Sie einfach &amp;ldquo;-lts&amp;rdquo; in die vmlinuz-linux.img und initramfs-linux.img ein, so dass sie vmlinuz-linux-lts.img und initramfs-linux-lts.img sind.&lt;/p>
&lt;h4 id="changing-the-systemd-boot-bootloader-for-the-lts-linux-kernels">Changing the systemd-boot bootloader for the LTS linux kernels&lt;/h4>
&lt;p>Wenn Sie aus meiner Arch-Installationsanleitung kommen, ist dies Ihr Bootloader.&lt;/p>
&lt;p>Bearbeiten Sie die Konfigurationsdatei:&lt;/p>
&lt;blockquote>
&lt;p>sudo nano /boot/loader/entries/arch.conf&lt;/p>
&lt;/blockquote>
&lt;p>Fügen Sie einfach &amp;ldquo;-lts&amp;rdquo; in die vmlinuz-linux.img und initramfs-linux.img ein, so dass sie vmlinuz-linux-lts.img und initramfs-linux-lts.img sind&lt;/p>
&lt;h3 id="4-installieren-von-yay-ein-einfacher-weg-aur-pakete-zu-installieren">4. Installieren von yay, ein einfacher Weg, AUR-Pakete zu installieren&lt;/h3>
&lt;p>Sie sollten es vorziehen, den ultraschnellen Pacman zur Installation der meisten Pakete zu verwenden, aber das Erstaunliche an Arch ist, dass Benutzer Millionen von benutzerdefinierten Paketen erstellen, die superleicht zu installieren sind. Sie können im Grunde jedes Programm, das Ihnen einfällt, in diesem Repo finden.&lt;/p>
&lt;p>git SVC installieren&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S git &lt;br>
mkdir ~/tmp &lt;br>
git-clone &lt;a href="https://aur.archlinux.org/yay-git.git">https://aur.archlinux.org/yay-git.git&lt;/a> ~/tmp/yay &lt;br>
cd ~/tmp/yay &lt;br>
makepkg -si&lt;/p>
&lt;/blockquote>
&lt;p>Jetzt können Sie all die schönen AUR-Pakete unter &lt;a href="https://aur.archlinux.org/packages/">https://aur.archlinux.org/packages/&lt;/a> durchstöbern oder einfach loslegen und tippen:&lt;/p>
&lt;blockquote>
&lt;p>yay -S [PAKET]&lt;/p>
&lt;/blockquote>
&lt;p>Um es zu installieren.&lt;/p>
&lt;h3 id="5-schließlich-die-eigentliche-cuda-cudnn-anaconda-installation-auf-der-sowohl-tensorflow-115-als-auch-20-läuft">5. Schließlich die eigentliche cuda, cudnn, anaconda-Installation, auf der sowohl TensorFlow 1.15 als auch 2.0 läuft&lt;/h3>
&lt;p>Installieren Sie Nvidia-Treiber, cuda, cudnn mit einem einfachen Befehl&lt;/p>
&lt;blockquote>
&lt;p>sudo pacman -S nvidia nvidia-utils cuda cudnn&lt;/p>
&lt;/blockquote>
&lt;p>Dies dauert einige Zeit, also holen Sie sich einen Kaffee oder fahren Sie mit den nächsten Schritten fort&lt;/p>
&lt;p>Anakonda herunterladen, ich mag Miniconda:&lt;/p>
&lt;blockquote>
&lt;p>wget &lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh">https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&lt;/a> ~/&lt;/p>
&lt;/blockquote>
&lt;p>Ausführbar machen und installieren&lt;/p>
&lt;blockquote>
&lt;p>cd ~/ &lt;br>
chmod +x ./Miniconda*.sh &lt;br>
./Miniconda*.sh&lt;/p>
&lt;/blockquote>
&lt;p>Lassen Sie einfach alles auf Standard.&lt;/p>
&lt;blockquote>
&lt;p>source ./bash_profile&lt;/p>
&lt;/blockquote>
&lt;p>Starten Sie Ihr System neu&lt;/p>
&lt;blockquote>
&lt;p>sudo reboot&lt;/p>
&lt;/blockquote>
&lt;p>Tensorflow installieren&lt;/p>
&lt;p>Jetzt ist es an der Zeit, sich zwischen TensorFlow für CPU oder GPU zu entscheiden. Ich werde mit der GPU-Option fortfahren, aber wenn Sie die CPU-Version laufen lassen wollen, entfernen Sie einfach das &amp;ldquo;-gpu&amp;rdquo; aus dem Paketnamen.&lt;/p>
&lt;h5 id="erstellen-sie-eine-anakonda-umgebung-für-tensorflow-20">Erstellen Sie eine Anakonda-Umgebung für Tensorflow 2.0&lt;/h5>
&lt;blockquote>
&lt;p>conda create &amp;ndash;name tf2.0 &lt;br>
conda activate tf2.0 &lt;br>
conda pip install &lt;br>
conda install tensorflow-gpu pandas numpy&lt;/p>
&lt;/blockquote>
&lt;p>Erledigt! Überprüfen Sie nun das Ergebnis mit:&lt;/p>
&lt;blockquote>
&lt;p>python &lt;br>
from tensorflow.python.client import device_lib &lt;br>
device_lib.list_local_devices()&lt;/p>
&lt;/blockquote>
&lt;p>Wenn das Ergebnis einen Gerätenamen wie diesen zeigt, sind Sie fertig!&lt;/p>
&lt;p>2018-05-01 05:25:25.929575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Gerät 0 mit Eigenschaften gefunden:
Name: GeForce GTX 3080 10GB Dur: &amp;hellip;&lt;/p>
&lt;h5 id="erstellen-sie-eine-anakonda-umgebung-für-tensorflow-115">Erstellen Sie eine Anakonda-Umgebung für Tensorflow 1.15&lt;/h5>
&lt;blockquote>
&lt;p>conda deactivate &lt;br>
conda create &amp;ndash;name tf1.15 &lt;br>
conda activate tf1.15 &lt;br>
conda install pip python==3.7 &lt;br>
conda install tensorflow-gpu===1.15&lt;/p>
&lt;/blockquote>
&lt;p>Und überprüfen Sie nochmals, ob alles funktioniert und Ihre gpu erkannt wird:&lt;/p>
&lt;blockquote>
&lt;p>python &lt;br>
from tensorflow.python.client import device_lib &lt;br>
device_lib.list_local_devices()&lt;/p>
&lt;/blockquote>
&lt;h3 id="6-umschalten-zwischen-tensorflow-115-und-tensorflow-20-auf-einem-gerät">6. Umschalten zwischen TensorFlow 1.15 und TensorFlow 2.0 auf einem Gerät!&lt;/h3>
&lt;p>Meiner Meinung nach ein Traum der wahr wird, wählen Sie einfach die Version 1.15 mit&lt;/p>
&lt;blockquote>
&lt;p>conda activate tf1.15&lt;/p>
&lt;/blockquote>
&lt;p>Und die TensorFlow 2.0 Version mit&lt;/p>
&lt;blockquote>
&lt;p>conda activate tf2.0&lt;/p>
&lt;/blockquote></description></item></channel></rss>