<!doctype html><html><head><title>Face Detection using MTCNN</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=description content="Face Detection using MTCNN – a guide for face extraction with a focus on speed"><link rel=canonical href=https://datafortress.cloud/blog/face-detection-using-mtcnn/><link rel=alternate href=https://datafortress.cloud/blog/face-detection-using-mtcnn/ hreflang=en-us><link rel=icon type=image/png href=https://datafortress.cloud/images/logo.webp><meta property="og:title" content="Face Detection using MTCNN"><meta property="og:description" content="Face Detection using MTCNN – a guide for face extraction with a focus on speed"><meta property="og:type" content="article"><meta property="og:url" content="https://datafortress.cloud/blog/face-detection-using-mtcnn/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-06-08T07:10:46+02:00"><meta property="article:modified_time" content="2022-07-07T21:15:36+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Face Detection using MTCNN"><meta name=twitter:description content="Face Detection using MTCNN – a guide for face extraction with a focus on speed"><script type=application/ld+json>{"@context":"https://schema.org/","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/datafortress.cloud\/blog\/face-detection-using-mtcnn\/"},"headline":"Face Detection using MTCNN","description":"Face Detection using MTCNN – a guide for face extraction with a focus on speed","image":[],"datePublished":"2022-06-08T07:10:46\u002b02:00","dateModified":"2022-07-07T21:15:36\u002b02:00","author":{"@type":"Person","name":"Justin Guese"},"publisher":{"@type":"Organization","name":"Datafortress.cloud","logo":{"@type":"ImageObject","url":"https:\/\/datafortress.cloud\/images\/logo.webp"}}}</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.13.1/css/all.css integrity=sha384-xxzQGERXS00kBmZW/6qxqJPyxW3UR0BPsL4c8ILaIWXva5kFi7TxkIIaMiKtqV1Q crossorigin=anonymous><link rel=stylesheet href=https://datafortress.cloud//css/style.css><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js></script></head><body><header><nav class=navbar><div class=navbar-brand><a class=navbar-item href=https://datafortress.cloud/ title=Datafortress.cloud><img src=https://datafortress.cloud/images/logo.webp alt="VR, Big-Data and Cloud for your Business."></a>
<a href=https://datafortress.cloud/ title="VR, Big-Data and Cloud for your Business." class=navbar-item><span class=is-size-4>Datafortress.cloud</span></a>
<a href=https://datafortress.cloud/contact class=navbar-item title="RSS feed"><span class="icon fa-lg has-text-dark"><i class="fa fa-envelope"></i></span></a>
<a href=https://datafortress.cloud/index.xml class=navbar-item title="RSS feed"><span class="icon fa-lg has-text-dark"><i class="fa fa-rss"></i></span></a>
<a role=button class=navbar-burger aria-label=menu aria-expanded=false><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a></div><div class="navbar-menu navbar-end"><a href=https://datafortress.cloud/products class=navbar-item>Products</a>
<a href=https://datafortress.cloud/services class=navbar-item>Services</a>
<a href=https://datafortress.cloud/project class=navbar-item>Portfolio</a>
<a href=https://datafortress.cloud/blog class=navbar-item>Blog</a>
<a href=https://datafortress.cloud/contact class=navbar-item>Contact</a>
<a href=https://datafortress.cloud/ class=navbar-item>about</a></div></nav><script>$(document).ready(function(){$(".navbar-burger").click(function(){$(".navbar-burger").toggleClass("is-active"),$(".navbar-menu").toggleClass("is-active")})})</script></header><main><article><div class="columns is-centered"><div class="column max-800px"><section class=section><h1 class="title is-2">Face Detection using MTCNN</h1><div class="columns is-tablet"><div class="columns is-mobile"><div class="column is-narrow"><figure class="image is-64x64"><img class=is-rounded src=https://datafortress.cloud/images/logo.webp alt="Justin Guese" title="Justin Guese"></figure></div><div class=column><p>Justin Guese |
<time datetime=2022-06-08>8 June 2022</time><br><time datetime=2022-07-07><span class=has-text-grey-light>Last Updated | 7 July 2022</span></time><br><span class=has-text-grey-light>1215 words | 6 minutes</span></p></div></div></div><div class="tags is-pulled-left"><a class="tag is-primary" href=https://datafortress.cloud/categories/computer-vision/>Computer Vision</a>
<a class="tag is-primary" href=https://datafortress.cloud/categories/big-data/>Big Data</a>
<a class="tag is-primary" href=https://datafortress.cloud/categories/machine-learning/>Machine Learning</a></div><div class="tags is-pulled-right"><a class="tag is-info" href=https://datafortress.cloud/tags/face-detection/>Face Detection</a>
<a class="tag is-info" href=https://datafortress.cloud/tags/neuronal-networks/>Neuronal Networks</a>
<a class="tag is-info" href=https://datafortress.cloud/tags/mtcnn/>MTCNN</a>
<a class="tag is-info" href=https://datafortress.cloud/tags/big-data/>Big Data</a>
<a class="tag is-info" href=https://datafortress.cloud/tags/machine-learning/>Machine Learning</a></div></section><section class=content><h1 id=what-is-mtcnn>What is MTCNN</h1><p>MTCNN is a python (pip) library written by <a href=https://github.com/ipazc/mtcnn>Github user ipacz</a>, which implements the [paper Zhang, Kaipeng et al. “Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.” IEEE Signal Processing Letters 23.10 (2016): 1499–1503. Crossref. Web](<a href=https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)>https://arxiv.org/abs/1604.02878%5D(https://arxiv.org/abs/1604.02878%20%22https://arxiv.org/abs/1604.02878)</a>.</p><p>In this paper, they propose a deep cascaded multi-task framework using different features of “sub-models” to each boost their correlating strengths.</p><p>MTCNN performs quite fast on a CPU, even though S3FD is still quicker running on a GPU – but that is a topic for another post.</p><p>This post uses code from the following two sources, check them out, they are interesting as well:</p><ul><li><a href=https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/ title=https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/>https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/</a></li><li><a href=https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution title=https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution>https://www.kaggle.com/timesler/fast-mtcnn-detector-55-fps-at-full-resolution</a></li></ul><h1 id=basic-usage-of-mtcnn>Basic usage of MTCNN</h1><p>Feel free to access the whole notebook via:</p><p><a href=https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up title=https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up>https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up</a></p><pre><code>git clone https://github.com/JustinGuese/mtcnn-face-extraction-eyes-mouth-nose-and-speeding-it-up
</code></pre><p>Luckily MTCNN is available as a pip package, meaning we can easily install it using</p><pre><code>pip install mtcnn
</code></pre><p>Now switching to Python/Jupyter Notebook we can check the installation with an import and quick verification:</p><pre><code>import mtcnn
# print version
print(mtcnn.__version__)
</code></pre><p>Afterwards, we are ready to load out test image using the matplotlib <a href=https://bit.ly/2vo3INw>imread function</a>.</p><pre><code>import matplotlib.pyplot as plt
# load image from file
filename = &quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&quot;
pixels = plt.imread(filename)
print(&quot;Shape of image/array:&quot;,pixels.shape)
imgplot = plt.imshow(pixels)
plt.show()
</code></pre><p>Now your output will look a lot like this:</p><pre><code>{'box': [1942, 716, 334, 415], 'confidence': 0.9999997615814209, 'keypoints': {'left_eye': (2053, 901), 'right_eye': (2205, 897), 'nose': (2139, 976), 'mouth_left': (2058, 1029), 'mouth_right': (2206, 1023)}}
{'box': [2084, 396, 37, 46], 'confidence': 0.9999206066131592, 'keypoints': {'left_eye': (2094, 414), 'right_eye': (2112, 414), 'nose': (2102, 426), 'mouth_left': (2095, 432), 'mouth_right': (2112, 431)}}
{'box': [1980, 381, 44, 59], 'confidence': 0.9998701810836792, 'keypoints': {'left_eye': (1997, 404), 'right_eye': (2019, 407), 'nose': (2010, 417), 'mouth_left': (1995, 425), 'mouth_right': (2015, 427)}}
{'box': [2039, 395, 39, 46], 'confidence': 0.9993435740470886, 'keypoints': {'left_eye': (2054, 409), 'right_eye': (2071, 415), 'nose': (2058, 422), 'mouth_left': (2048, 425), 'mouth_right': (2065, 431)}}
</code></pre><p>What does this tell us? A lot of it is self-explanatory, but it basically returns coordinates, or the pixel values of a rectangle where the MTCNN algorithm detected faces. The “box” value above returns the location of the whole face, followed by a “confidence” level.</p><p>If you want to do more advanced extractions or algorithms, you will have access to other facial landmarks, called “keypoints” as well. Namely the MTCNN model located the eyes, mouth and nose as well!</p><h2 id=drawing-a-box-around-faces>Drawing a box around faces</h2><p>To demonstrate this even better let us draw a box around the face using matplotlib:</p><pre><code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height, fill=False, color='green')
# draw the box
ax.add_patch(rect)
# show the plot
plt.show()

# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
</code></pre><p><img src=/images/index-1-150x150.webp alt></p><h2 id=displaying-eyes-mouth-and-nose-around-faces>Displaying eyes, mouth, and nose around faces</h2><p>Now let us take a look at the aforementioned “keypoints” that the MTCNN model returned.</p><p>We will now use these to graph the nose, mouth, and eyes as well.<br>We will add the following code snippet to our code above:</p><pre><code># draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='orange')
ax.add_patch(dot)
</code></pre><p>With the full code from above looking like this:</p><pre><code># draw an image with detected objects
def draw_facebox(filename, result_list):
# load the image
data = plt.imread(filename)
# plot the image
plt.imshow(data)
# get the context for drawing boxes
ax = plt.gca()
# plot each box
for result in result_list:
# get coordinates
x, y, width, height = result['box']
# create the shape
rect = plt.Rectangle((x, y), width, height,fill=False, color='orange')
# draw the box
ax.add_patch(rect)
# draw the dots
for key, value in result['keypoints'].items():
# create and draw dot
dot = plt.Circle(value, radius=20, color='red')
ax.add_patch(dot)
# show the plot
plt.show()

# filename = 'test1.webp' # filename is defined above, otherwise uncomment
# load image from file
# pixels = plt.imread(filename) # defined above, otherwise uncomment
# detector is defined above, otherwise uncomment
#detector = mtcnn.MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_facebox(filename, faces)
</code></pre><p><img src=/images/index2-150x150.webp alt></p><h2 id=advanced-mtcnn-speed-it-up-x100>Advanced MTCNN: Speed it up (\~x100)!</h2><p>Now let us come to the interesting part. If you are going to process millions of pictures you will need to speed up MTCNN, otherwise, you will either fall asleep or your CPU will burn before it will be done.</p><p>But what exactly are we talking about? If you are running the above code it will take around one second, meaning we will process around one picture per second. If you are running MTCNN on a GPU and use the sped-up version it will achieve around 60-100 pictures/frames a second. That is a boost of up to <strong>100 times</strong>!</p><p>If you are for example going to extract all faces of a movie, where you will extract 10 faces per second (one second of the movie has on average around 24 frames, so every second frame) it will be 10 * 60 (seconds) * 120 (minutes) = 72,000 frames.</p><p>Meaning if it takes one second to process one frame it will take 72,000 * 1 (seconds) = 72,000s / 60s = 1,200m = <strong>20 hours</strong></p><p>With the sped-up version of MTCNN this task will take 72,000 (frames) / 100 (frames/sec) = 720 seconds = <strong>12 minutes</strong>!</p><p>To use MTCNN on a GPU you will need to set up CUDA, cudnn, pytorch and so on. <a href=https://pytorch.org/get-started/locally/>Pytorch wrote a good tutorial about that part</a>.</p><p>Once installed we will do the necessary imports as follows:</p><pre><code>from facenet_pytorch import MTCNN
from PIL import Image
import torch
from imutils.video import FileVideoStream
import cv2
import time
import glob
from tqdm.notebook import tqdm

device = 'cuda' if torch.cuda.is_available() else 'cpu'

filenames = [&quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&quot;,&quot;glediston-bastos-ZtmmR9D_2tA-unsplash.webp&quot;]
</code></pre><p>See how we defined the device in the code above? You will be able to run everything on a CPU as well if you do not want or can set up CUDA.</p><p>Next, we will define the extractor:</p><pre><code># define our extractor
fast_mtcnn = FastMTCNN(
stride=4,
resize=0.5,
margin=14,
factor=0.6,
keep_all=True,
device=device
)
</code></pre><p>In this snippet, we pass along some parameters, where we for example only use half of the image size, which is one of the main impact factors for speeding it up.</p><p>And finally, let us run the face extraction script:</p><pre><code>def run_detection(fast_mtcnn, filenames):
frames = []
frames_processed = 0
faces_detected = 0
batch_size = 60
start = time.time()

for filename in tqdm(filenames):

v_cap = FileVideoStream(filename).start()
v_len = int(v_cap.stream.get(cv2.CAP_PROP_FRAME_COUNT))

for j in range(v_len):

frame = v_cap.read()
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
frames.append(frame)

if len(frames) &gt;= batch_size or j == v_len - 1:

faces = fast_mtcnn(frames)

frames_processed += len(frames)
faces_detected += len(faces)
frames = []

print(
f'Frames per second: {frames_processed / (time.time() - start):.3f},',
f'faces detected: {faces_detected}\r',
end=''
)

v_cap.stop()

run_detection(fast_mtcnn, filenames)
</code></pre><p><img src=/images/teslap100frames.webp alt></p><p>The above image shows the output of the code running on an NVIDIA Tesla P100, so depending on the source material, GPU and processor you might experience better or worse performance.</p></section></div></div></article><aside class="columns is-centered"><div class="column max-800px"><div class=content><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//datafortess-cloud.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></aside></main><footer class=footer><div class="columns has-text-centered"><div class="column is-one-third is-narrow"><p>Social media</p></div><div class="column is-one-third is-narrow"><i class='fab fa-creative-commons' aria-hidden=true></i> This work is licensed under a <a rel=license href=https://creativecommons.org/licenses/by-sa/4.0/ target=_blank>Creative Commons Attribution-ShareAlike 4.0 International</a> License.</div><div class="column is-one-third is-narrow"><p>Powered by</p><a href=https://gohugo.io/ target=_blank><img src=https://datafortress.cloud/images/hugo-logo.png alt=Hugo width=64 height=12 title="Powered by Hugo"></a> &nbsp; &nbsp;
<a href=https://bulma.io/ target=_blank><img src=https://datafortress.cloud/images/made-with-bulma.png alt=Bulma width=64 height=16 title="Powered by Bulma CSS"></a>&nbsp; &nbsp;
<a href=https://www.gnu.org/software/emacs/ target=_blank><img src=https://datafortress.cloud/images/emacs-logo.png alt=Emacs width=16 height=16 title="Written in Emacs"></a></div></div></footer></body></html>